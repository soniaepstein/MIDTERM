{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28f44eee",
   "metadata": {},
   "source": [
    "# Guidelines\n",
    "\n",
    "**Step 1. Write a program that computes the following items for each document.**\n",
    "\n",
    "- A type-token ratio (The total # of types divided by the total # of tokens).\n",
    "    - (total number of UNIQUE words (types) divided by the total number of words (tokens))\n",
    "- An average word length (i.e., number of characters in a word)\n",
    "- The total number of words\n",
    "- The top 10 most frequently used words (excluding function words)\n",
    "- The top 10 most frequently used bigrams, and trigrams (bigrams and trigrams should include function words)\n",
    "\n",
    "Note: Include an appropriate level of comments in your code so that I can follow your program easily. You may add some print statements to show that each segment of your program is working.\n",
    "\n",
    "**Step 2. Based on the program you wrote for Step 1, write a program that computes the following items for each group:**\n",
    "\n",
    "- An average type-token ratio (TTR).\n",
    "- An average word length\n",
    "- An average number of words in a document\n",
    "- The top 10 most frequently used words (excluding function words)\n",
    "- The top 10 most frequently used bigrams and trigrams. (bigrams and trigrams should include function words)\n",
    "\n",
    "**Step 3. Based on the programs you wrote for Step 1 and 2, write a program that automatically generates a spreadsheet (.CSV file) that summarizes the results for each group. Each spreadsheet should look like the spreadsheet below.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f792f943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meet me at midnight...\n",
      "Staring at the ceiling with you\n",
      "Oh, you don't ever say too much\n",
      "And you don't really read into\n",
      "My melancholia\n",
      "I been under scrutiny\n",
      "You handle it beautifully\n",
      "All this shit is new to me\n",
      "I feel the lavender haze creeping up on me\n",
      "Surreal\n",
      "I'm damned if I do give a damn what people say\n",
      "No deal\n",
      "The 1950s shit they want from me\n",
      "I just wanna stay in that lavender haze\n",
      "All they keep asking me\n",
      "Is if I'm gonna be your bride\n",
      "The only kinda girl they see\n",
      "Is a one night or a wife\n",
      "I find it dizzying\n",
      "They're bringing up my history\n",
      "But you weren't even listening\n",
      "I feel the lavender haze creeping up on me\n",
      "Surreal\n",
      "I'm damned if I do give a damn what people say\n",
      "No deal\n",
      "The 1950s shit they want from me\n",
      "I just wanna stay in that lavender haze\n",
      "That lavender haze\n",
      "Talk your talk and go viral\n",
      "I just need this love spiral\n",
      "Get it off your chest\n",
      "Get it off my desk\n",
      "Talk your talk and go viral\n",
      "I just need this love spiral\n",
      "Get it off your chest\n",
      "Get it off my desk\n",
      "I feel the lavender haze creeping up on me\n",
      "Surreal\n",
      "I'm damned if I do give a damn what people say\n",
      "No deal\n",
      "The 1950s shit they want from me\n",
      "I just wanna stay in that lavender haze\n",
      "Get it off your chest\n",
      "Get it off my desk\n",
      "That lavender haze, I just wanna stay\n",
      "I just wanna stay in that lavender haze\n"
     ]
    }
   ],
   "source": [
    "fhandle = open(\"Songs/Midnights (2022)/ts10t1_lavender_haze.txt\") #path format\n",
    "text = fhandle.read()\n",
    "print(text) #prints song lyrics!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2146c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import bigrams\n",
    "from nltk import trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a539f187",
   "metadata": {},
   "source": [
    "### Song File Names LIST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4869786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Songs/Taylor Swift (2006)/ts1t3_teardrops_on_my_guitar.txt\n",
      "Songs/Taylor Swift (2006)/ts1t1_tim_mcgraw.txt\n",
      "Songs/Taylor Swift (2006)/ts1t8_stay_beautiful.txt\n",
      "Songs/Taylor Swift (2006)/ts1t4_a_place_in_this_world.txt\n",
      "Songs/Taylor Swift (2006)/ts1t9_shouldve_said_no.txt\n",
      "Songs/Taylor Swift (2006)/ts1t7_tied_together_with_a_smile.txt\n",
      "Songs/Taylor Swift (2006)/ts1t10_marys_song.txt\n",
      "Songs/Taylor Swift (2006)/ts1t2_picture_to_burn.txt\n",
      "Songs/Taylor Swift (2006)/ts1t11_our_song.txt\n",
      "Songs/Taylor Swift (2006)/ts1t6_the_outside.txt\n",
      "Songs/Taylor Swift (2006)/ts1t5_cold_as_you.txt\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "path = pathlib.Path(\"Songs/Taylor Swift (2006)\")\n",
    "files = path.glob(\"*.txt\")\n",
    "for file in files:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "657e0838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Songs/Midnights (2022)/ts10t8_vigilante_shit.txt\n",
      "Songs/Midnights (2022)/ts10t10_labyrinth.txt\n",
      "Songs/Midnights (2022)/ts10t13_mastermind.txt\n",
      "Songs/Midnights (2022)/ts10t4_snow_on_the_beach.txt\n",
      "Songs/Midnights (2022)/ts10t9_bejeweled.txt\n",
      "Songs/Midnights (2022)/ts10t11_karma.txt\n",
      "Songs/Midnights (2022)/ts10t3_anti_hero.txt\n",
      "Songs/Midnights (2022)/ts10t1_lavender_haze.txt\n",
      "Songs/Midnights (2022)/ts10t2_maroon.txt\n",
      "Songs/Midnights (2022)/ts10t7_question.txt\n",
      "Songs/Midnights (2022)/ts10t5_youre_on_your_own_kid.txt\n",
      "Songs/Midnights (2022)/ts10t12_sweet_nothing.txt\n",
      "Songs/Midnights (2022)/ts10t6_midnight_rain.txt\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "path = pathlib.Path(\"Songs/Midnights (2022)\")\n",
    "files = path.glob(\"*.txt\")\n",
    "for file in files:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949ac9de",
   "metadata": {},
   "source": [
    "### COUNT WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2bcd66d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'I': 21, 'was': 5, \"ridin'\": 2, 'shotgun': 2, 'with': 2, 'my': 5, 'hair': 2, 'undone': 2, 'In': 2, 'the': 18, 'front': 3, 'seat': 2, 'of': 2, 'his': 4, 'car': 2, \"He's\": 1, 'got': 4, 'a': 3, 'one-hand': 1, 'feel': 1, 'on': 10, 'steering': 1, 'wheel': 1, 'The': 4, 'other': 1, 'heart': 1, 'look': 1, 'around,': 1, 'turn': 1, 'radio': 2, 'down': 2, 'He': 1, 'says,': 1, '\"Baby,': 1, 'is': 7, 'something': 2, 'wrong?\"': 1, 'say,': 1, '\"Nothing,': 1, 'just': 1, \"thinkin'\": 1, 'how': 1, 'we': 1, \"don't\": 4, 'have': 1, 'song\"': 1, 'And': 7, 'he': 6, 'says': 1, 'Our': 5, 'song': 8, 'slamming': 3, 'screen': 3, 'door': 3, \"Sneakin'\": 3, 'out': 3, 'late,': 6, 'tapping': 3, 'your': 4, 'window': 3, 'When': 3, \"we're\": 3, 'phone,': 3, 'and': 12, 'you': 4, 'talk': 2, 'real': 3, 'slow': 3, \"'Cause\": 4, \"it's\": 3, 'mama': 3, 'know': 3, 'way': 4, 'laugh': 2, 'first': 3, 'date,': 3, '\"Man,': 3, \"didn't\": 4, 'kiss': 3, 'her,': 1, 'should': 3, 'have\"': 3, 'when': 3, 'home,': 3, \"'fore\": 3, 'said,': 3, '\"Amen\"': 3, 'Asking': 2, 'God': 3, 'if': 3, 'could': 3, 'play': 4, 'it': 4, 'again': 3, \"walkin'\": 1, 'up': 1, 'porch': 1, 'steps': 1, 'after': 1, 'everything': 1, 'that': 2, 'day': 1, 'Had': 1, 'gone': 1, 'all': 2, 'wrong': 1, 'been': 1, 'trampled': 1, 'lost': 1, 'thrown': 1, 'away': 1, 'Got': 1, 'to': 4, 'hallway,': 1, 'well': 1, \"lovin'\": 1, 'bed': 1, 'almost': 1, 'notice': 1, 'roses': 1, 'note': 1, 'said': 1, 'her': 1, \"I've\": 1, 'heard': 1, 'every': 1, 'album,': 1, 'listened': 1, 'Waited': 1, 'for': 1, 'come': 1, 'along': 1, 'That': 1, 'as': 2, 'good': 1, 'our': 3, 'talks': 1, 'laughs': 1, 'him,': 1, \"Askin'\": 1, 'again,': 1, 'Oh,': 2, 'yeah': 2, 'oh': 1, 'grabbed': 1, 'pen': 1, 'an': 1, 'old': 1, 'napkin': 1, 'wrote': 1}\n"
     ]
    }
   ],
   "source": [
    "def word_count(a):\n",
    "    fhandle = open(a)\n",
    "    s = fhandle.read()\n",
    "    words = s.split()\n",
    "    d = dict()\n",
    "    for word in words:\n",
    "        if word not in d:\n",
    "            d[word] = 1\n",
    "        else:\n",
    "            d[word] += 1\n",
    "    return d\n",
    "\n",
    "d = word_count(\"Songs/Taylor Swift (2006)/ts1t11_our_song.txt\")\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c70219c",
   "metadata": {},
   "source": [
    "### WORD LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01d8e0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bittersweet\n",
      "hollow-eyed\n"
     ]
    }
   ],
   "source": [
    "def make_word_list(fpath):\n",
    "    with open(fpath, errors=\"ignore\") as fin:\n",
    "        text  = fin.read()\n",
    "        words = text.split()\n",
    "    return words\n",
    "\n",
    "\n",
    "words = make_word_list(\"Songs/Taylor Swift (2006)/ts1t1_tim_mcgraw.txt\")\n",
    "longest = None\n",
    "for item in words:                                     \n",
    "    if longest is None or len(item) > len(longest) : \n",
    "        longest = item \n",
    "print(longest)\n",
    "\n",
    "words = make_word_list(\"Songs/Midnights (2022)/ts10t2_maroon.txt\")\n",
    "longest = None\n",
    "for item in words:                                     \n",
    "    if longest is None or len(item) > len(longest) : \n",
    "        longest = item \n",
    "print(longest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0514cd78",
   "metadata": {},
   "source": [
    "# MIDTERM CODES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5078a147",
   "metadata": {},
   "source": [
    "## 1.1 - TTR for each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf1ee4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALBUM 1, TRACK 1: The type-token ratio for 'Tim McGraw' is 0.3551912568306011.\n",
      "ALBUM 1, TRACK 2: The type-token ratio for 'Picture to Burn' is 0.41379310344827586.\n",
      "ALBUM 1, TRACK 3: The type-token ratio for 'Teardrops On My Guitar' is 0.4895833333333333.\n",
      "ALBUM 1, TRACK 4: The type-token ratio for 'A Place In This World' is 0.3948497854077253.\n",
      "ALBUM 1, TRACK 5: The type-token ratio for 'Cold As You' is 0.46530612244897956.\n",
      "ALBUM 1, TRACK 6: The type-token ratio for 'The Outside' is 0.4345991561181435.\n",
      "ALBUM 1, TRACK 7: The type-token ratio for 'Tied Together With a Smile' is 0.4065040650406504.\n",
      "ALBUM 1, TRACK 8: The type-token ratio for 'Stay Beautiful' is 0.49063670411985016.\n",
      "ALBUM 1, TRACK 9: The type-token ratio for 'Should've Said No' is 0.3457142857142857.\n",
      "ALBUM 1, TRACK 10: The type-token ratio for 'Mary's Song' is 0.48787878787878786.\n",
      "ALBUM 1, TRACK 11: The type-token ratio for 'Our Song' is 0.40540540540540543.\n",
      "\n",
      "ALBUM 10, TRACK 1: The type-token ratio for 'Lavender Haze' is 0.3849056603773585.\n",
      "ALBUM 10, TRACK 2: The type-token ratio for 'Maroon' is 0.38571428571428573.\n",
      "ALBUM 10, TRACK 3: The type-token ratio for 'Anti-Hero' is 0.47289156626506024.\n",
      "ALBUM 10, TRACK 4: The type-token ratio for 'Snow On The Beach' is 0.37567567567567567.\n",
      "ALBUM 10, TRACK 5: The type-token ratio for 'You're On Your Own, Kid' is 0.59.\n",
      "ALBUM 10, TRACK 6: The type-token ratio for 'Midnight Rain' is 0.41935483870967744.\n",
      "ALBUM 10, TRACK 7: The type-token ratio for 'Question...?' is 0.39631336405529954.\n",
      "ALBUM 10, TRACK 8: The type-token ratio for 'Vigilante Shit' is 0.5134099616858238.\n",
      "ALBUM 10, TRACK 9: The type-token ratio for 'Bejeweled' is 0.34523809523809523.\n",
      "ALBUM 10, TRACK 10: The type-token ratio for 'Labyrinth' is 0.3308550185873606.\n",
      "ALBUM 10, TRACK 11: The type-token ratio for 'Karma' is 0.39313984168865435.\n",
      "ALBUM 10, TRACK 12: The type-token ratio for 'Sweet Nothing' is 0.4349442379182156.\n",
      "ALBUM 10, TRACK 13: The type-token ratio for 'Mastermind' is 0.42857142857142855.\n"
     ]
    }
   ],
   "source": [
    "#TTR PER SONG\n",
    "\n",
    "def make_word_list(fpath):\n",
    "    with open(fpath, errors=\"ignore\") as fin:\n",
    "        text  = fin.read()\n",
    "        words = text.split()\n",
    "    return words\n",
    "\n",
    "def get_ttr(list_of_words):\n",
    "    unique_words = []\n",
    "    \n",
    "    for word in list_of_words:\n",
    "        if word not in unique_words:\n",
    "            unique_words.append(word)\n",
    "               \n",
    "    ttr = len(unique_words)/len(list_of_words)\n",
    "    return ttr\n",
    "\n",
    "words = make_word_list(\"Songs/Taylor Swift (2006)/ts1t1_tim_mcgraw.txt\")\n",
    "ttr = get_ttr(words)\n",
    "print(f\"ALBUM 1, TRACK 1: The type-token ratio for 'Tim McGraw' is {ttr}.\")\n",
    "\n",
    "words = make_word_list(\"Songs/Taylor Swift (2006)/ts1t2_picture_to_burn.txt\")\n",
    "ttr = get_ttr(words)\n",
    "print(f\"ALBUM 1, TRACK 2: The type-token ratio for 'Picture to Burn' is {ttr}.\")\n",
    "\n",
    "words = make_word_list(\"Songs/Taylor Swift (2006)/ts1t3_teardrops_on_my_guitar.txt\")\n",
    "ttr = get_ttr(words)\n",
    "print(f\"ALBUM 1, TRACK 3: The type-token ratio for 'Teardrops On My Guitar' is {ttr}.\")\n",
    "\n",
    "words = make_word_list(\"Songs/Taylor Swift (2006)/ts1t4_a_place_in_this_world.txt\")\n",
    "ttr = get_ttr(words)\n",
    "print(f\"ALBUM 1, TRACK 4: The type-token ratio for 'A Place In This World' is {ttr}.\")\n",
    "\n",
    "words = make_word_list(\"Songs/Taylor Swift (2006)/ts1t5_cold_as_you.txt\")\n",
    "ttr = get_ttr(words)\n",
    "print(f\"ALBUM 1, TRACK 5: The type-token ratio for 'Cold As You' is {ttr}.\")\n",
    "\n",
    "words = make_word_list(\"Songs/Taylor Swift (2006)/ts1t6_the_outside.txt\")\n",
    "ttr = get_ttr(words)\n",
    "print(f\"ALBUM 1, TRACK 6: The type-token ratio for 'The Outside' is {ttr}.\")\n",
    "\n",
    "words = make_word_list(\"Songs/Taylor Swift (2006)/ts1t7_tied_together_with_a_smile.txt\")\n",
    "ttr = get_ttr(words)\n",
    "print(f\"ALBUM 1, TRACK 7: The type-token ratio for 'Tied Together With a Smile' is {ttr}.\")\n",
    "\n",
    "words = make_word_list(\"Songs/Taylor Swift (2006)/ts1t8_stay_beautiful.txt\")\n",
    "ttr = get_ttr(words)\n",
    "print(f\"ALBUM 1, TRACK 8: The type-token ratio for 'Stay Beautiful' is {ttr}.\")\n",
    "\n",
    "words = make_word_list(\"Songs/Taylor Swift (2006)/ts1t9_shouldve_said_no.txt\")\n",
    "ttr = get_ttr(words)\n",
    "print(f\"ALBUM 1, TRACK 9: The type-token ratio for 'Should've Said No' is {ttr}.\")\n",
    "\n",
    "words = make_word_list(\"Songs/Taylor Swift (2006)/ts1t10_marys_song.txt\")\n",
    "ttr = get_ttr(words)\n",
    "print(f\"ALBUM 1, TRACK 10: The type-token ratio for 'Mary's Song' is {ttr}.\")\n",
    "\n",
    "words = make_word_list(\"Songs/Taylor Swift (2006)/ts1t11_our_song.txt\")\n",
    "ttr = get_ttr(words)\n",
    "print(f\"ALBUM 1, TRACK 11: The type-token ratio for 'Our Song' is {ttr}.\")\n",
    "\n",
    "print (    )\n",
    "\n",
    "words = make_word_list(\"Songs/Midnights (2022)/ts10t1_lavender_haze.txt\")\n",
    "ttr = get_ttr(words)\n",
    "print(f\"ALBUM 10, TRACK 1: The type-token ratio for 'Lavender Haze' is {ttr}.\")\n",
    "\n",
    "words = make_word_list(\"Songs/Midnights (2022)/ts10t2_maroon.txt\")\n",
    "ttr = get_ttr(words)\n",
    "print(f\"ALBUM 10, TRACK 2: The type-token ratio for 'Maroon' is {ttr}.\")\n",
    "\n",
    "words = make_word_list(\"Songs/Midnights (2022)/ts10t3_anti_hero.txt\")\n",
    "ttr = get_ttr(words)\n",
    "print(f\"ALBUM 10, TRACK 3: The type-token ratio for 'Anti-Hero' is {ttr}.\")\n",
    "\n",
    "words = make_word_list(\"Songs/Midnights (2022)/ts10t4_snow_on_the_beach.txt\")\n",
    "ttr = get_ttr(words)\n",
    "print(f\"ALBUM 10, TRACK 4: The type-token ratio for 'Snow On The Beach' is {ttr}.\")\n",
    "\n",
    "words = make_word_list(\"Songs/Midnights (2022)/ts10t5_youre_on_your_own_kid.txt\")\n",
    "ttr = get_ttr(words)\n",
    "print(f\"ALBUM 10, TRACK 5: The type-token ratio for 'You're On Your Own, Kid' is {ttr}.\")\n",
    "\n",
    "words = make_word_list(\"Songs/Midnights (2022)/ts10t6_midnight_rain.txt\")\n",
    "ttr = get_ttr(words)\n",
    "print(f\"ALBUM 10, TRACK 6: The type-token ratio for 'Midnight Rain' is {ttr}.\")\n",
    "\n",
    "words = make_word_list(\"Songs/Midnights (2022)/ts10t7_question.txt\")\n",
    "ttr = get_ttr(words)\n",
    "print(f\"ALBUM 10, TRACK 7: The type-token ratio for 'Question...?' is {ttr}.\")\n",
    "\n",
    "words = make_word_list(\"Songs/Midnights (2022)/ts10t8_vigilante_shit.txt\")\n",
    "ttr = get_ttr(words)\n",
    "print(f\"ALBUM 10, TRACK 8: The type-token ratio for 'Vigilante Shit' is {ttr}.\")\n",
    "\n",
    "words = make_word_list(\"Songs/Midnights (2022)/ts10t9_bejeweled.txt\")\n",
    "ttr = get_ttr(words)\n",
    "print(f\"ALBUM 10, TRACK 9: The type-token ratio for 'Bejeweled' is {ttr}.\")\n",
    "\n",
    "words = make_word_list(\"Songs/Midnights (2022)/ts10t10_labyrinth.txt\")\n",
    "ttr = get_ttr(words)\n",
    "print(f\"ALBUM 10, TRACK 10: The type-token ratio for 'Labyrinth' is {ttr}.\")\n",
    "\n",
    "words = make_word_list(\"Songs/Midnights (2022)/ts10t11_karma.txt\")\n",
    "ttr = get_ttr(words)\n",
    "print(f\"ALBUM 10, TRACK 11: The type-token ratio for 'Karma' is {ttr}.\")\n",
    "\n",
    "words = make_word_list(\"Songs/Midnights (2022)/ts10t12_sweet_nothing.txt\")\n",
    "ttr = get_ttr(words)\n",
    "print(f\"ALBUM 10, TRACK 12: The type-token ratio for 'Sweet Nothing' is {ttr}.\")\n",
    "\n",
    "words = make_word_list(\"Songs/Midnights (2022)/ts10t13_mastermind.txt\")\n",
    "ttr = get_ttr(words)\n",
    "print(f\"ALBUM 10, TRACK 13: The type-token ratio for 'Mastermind' is {ttr}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c166b6a",
   "metadata": {},
   "source": [
    "## 1.2 - Average word length for each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49f937b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALBUM 1, TRACK 1: Average word length for 'Tim McGraw' is 3.849726775956284 characters.\n",
      "ALBUM 1, TRACK 2: Average word length for 'Picture to Burn' is 4.244827586206896 characters.\n",
      "ALBUM 1, TRACK 3: Average word length for 'Teardrops On My Guitar' is 3.7916666666666665 characters.\n",
      "ALBUM 1, TRACK 4: Average word length for 'A Place In This World' is 3.587982832618026 characters.\n",
      "ALBUM 1, TRACK 5: Average word length for 'Cold As You' is 3.8816326530612244 characters.\n",
      "ALBUM 1, TRACK 6: Average word length for 'The Outside' is 3.6160337552742616 characters.\n",
      "ALBUM 1, TRACK 7: Average word length for 'Tied Together With a Smile' is 4.138211382113822 characters.\n",
      "ALBUM 1, TRACK 8: Average word length for 'Stay Beautiful' is 4.284644194756554 characters.\n",
      "ALBUM 1, TRACK 9: Average word length for 'Should've Said No' is 4.142857142857143 characters.\n",
      "ALBUM 1, TRACK 10: Average word length for 'Mary's Song' is 3.757575757575758 characters.\n",
      "ALBUM 1, TRACK 11: Average word length for 'Our Song' is 3.9324324324324325 characters.\n",
      "\n",
      "ALBUM 10, TRACK 1: Average word length for 'Lavender Haze' is 3.7849056603773583 characters.\n",
      "ALBUM 10, TRACK 2: Average word length for 'Maroon' is 4.057142857142857 characters.\n",
      "ALBUM 10, TRACK 3: Average word length for 'Anti-Hero' is 4.340361445783133 characters.\n",
      "ALBUM 10, TRACK 4: Average word length for 'Snow On The Beach' is 4.2405405405405405 characters.\n",
      "ALBUM 10, TRACK 5: Average word length for 'You're On Your Own, Kid' is 4.173333333333333 characters.\n",
      "ALBUM 10, TRACK 6: Average word length for 'Midnight Rain' is 4.172043010752688 characters.\n",
      "ALBUM 10, TRACK 7: Average word length for 'Question...?' is 4.027649769585254 characters.\n",
      "ALBUM 10, TRACK 8: Average word length for 'Vigilante Shit' is 4.099616858237548 characters.\n",
      "ALBUM 10, TRACK 9: Average word length for 'Bejeweled' is 3.919047619047619 characters.\n",
      "ALBUM 10, TRACK 10: Average word length for 'Labyrinth' is 4.197026022304833 characters.\n",
      "ALBUM 10, TRACK 11: Average word length for 'Karma' is 4.073878627968337 characters.\n",
      "ALBUM 10, TRACK 12: Average word length for 'Sweet Nothing' is 4.5427509293680295 characters.\n",
      "ALBUM 10, TRACK 13: Average word length for 'Mastermind' is 3.875 characters.\n"
     ]
    }
   ],
   "source": [
    "#AVG WORD LENGTH PER SONG\n",
    "\n",
    "def make_word_list(fpath):\n",
    "    with open(fpath, errors=\"ignore\") as fin:\n",
    "        text  = fin.read()\n",
    "        words = text.split()\n",
    "    return words\n",
    "\n",
    "def avg_word_length(fpath):\n",
    "    words = make_word_list(fpath)\n",
    "    average = sum(len(word) for word in words) / len(words)\n",
    "    return average\n",
    "\n",
    "r = avg_word_length(\"Songs/Taylor Swift (2006)/ts1t1_tim_mcgraw.txt\")\n",
    "print(f\"ALBUM 1, TRACK 1: Average word length for 'Tim McGraw' is {r} characters.\")\n",
    "\n",
    "r = avg_word_length(\"Songs/Taylor Swift (2006)/ts1t2_picture_to_burn.txt\")\n",
    "print(f\"ALBUM 1, TRACK 2: Average word length for 'Picture to Burn' is {r} characters.\")\n",
    "\n",
    "r = avg_word_length(\"Songs/Taylor Swift (2006)/ts1t3_teardrops_on_my_guitar.txt\")\n",
    "print(f\"ALBUM 1, TRACK 3: Average word length for 'Teardrops On My Guitar' is {r} characters.\")\n",
    "\n",
    "r = avg_word_length(\"Songs/Taylor Swift (2006)/ts1t4_a_place_in_this_world.txt\")\n",
    "print(f\"ALBUM 1, TRACK 4: Average word length for 'A Place In This World' is {r} characters.\")\n",
    "\n",
    "r = avg_word_length(\"Songs/Taylor Swift (2006)/ts1t5_cold_as_you.txt\")\n",
    "print(f\"ALBUM 1, TRACK 5: Average word length for 'Cold As You' is {r} characters.\")\n",
    "\n",
    "r = avg_word_length(\"Songs/Taylor Swift (2006)/ts1t6_the_outside.txt\")\n",
    "print(f\"ALBUM 1, TRACK 6: Average word length for 'The Outside' is {r} characters.\")\n",
    "\n",
    "r = avg_word_length(\"Songs/Taylor Swift (2006)/ts1t7_tied_together_with_a_smile.txt\")\n",
    "print(f\"ALBUM 1, TRACK 7: Average word length for 'Tied Together With a Smile' is {r} characters.\")\n",
    "\n",
    "r = avg_word_length(\"Songs/Taylor Swift (2006)/ts1t8_stay_beautiful.txt\")\n",
    "print(f\"ALBUM 1, TRACK 8: Average word length for 'Stay Beautiful' is {r} characters.\")\n",
    "\n",
    "r = avg_word_length(\"Songs/Taylor Swift (2006)/ts1t9_shouldve_said_no.txt\")\n",
    "print(f\"ALBUM 1, TRACK 9: Average word length for 'Should've Said No' is {r} characters.\")\n",
    "\n",
    "r = avg_word_length(\"Songs/Taylor Swift (2006)/ts1t10_marys_song.txt\")\n",
    "print(f\"ALBUM 1, TRACK 10: Average word length for 'Mary's Song' is {r} characters.\")\n",
    "\n",
    "r = avg_word_length(\"Songs/Taylor Swift (2006)/ts1t11_our_song.txt\")\n",
    "print(f\"ALBUM 1, TRACK 11: Average word length for 'Our Song' is {r} characters.\")\n",
    "\n",
    "print(   )\n",
    "\n",
    "r = avg_word_length(\"Songs/Midnights (2022)/ts10t1_lavender_haze.txt\")\n",
    "print(f\"ALBUM 10, TRACK 1: Average word length for 'Lavender Haze' is {r} characters.\")\n",
    "\n",
    "r = avg_word_length(\"Songs/Midnights (2022)/ts10t2_maroon.txt\")\n",
    "print(f\"ALBUM 10, TRACK 2: Average word length for 'Maroon' is {r} characters.\")\n",
    "\n",
    "r = avg_word_length(\"Songs/Midnights (2022)/ts10t3_anti_hero.txt\")\n",
    "print(f\"ALBUM 10, TRACK 3: Average word length for 'Anti-Hero' is {r} characters.\")\n",
    "\n",
    "r = avg_word_length(\"Songs/Midnights (2022)/ts10t4_snow_on_the_beach.txt\")\n",
    "print(f\"ALBUM 10, TRACK 4: Average word length for 'Snow On The Beach' is {r} characters.\")\n",
    "\n",
    "r = avg_word_length(\"Songs/Midnights (2022)/ts10t5_youre_on_your_own_kid.txt\")\n",
    "print(f\"ALBUM 10, TRACK 5: Average word length for 'You're On Your Own, Kid' is {r} characters.\")\n",
    "\n",
    "r = avg_word_length(\"Songs/Midnights (2022)/ts10t6_midnight_rain.txt\")\n",
    "print(f\"ALBUM 10, TRACK 6: Average word length for 'Midnight Rain' is {r} characters.\")\n",
    "\n",
    "r = avg_word_length(\"Songs/Midnights (2022)/ts10t7_question.txt\")\n",
    "print(f\"ALBUM 10, TRACK 7: Average word length for 'Question...?' is {r} characters.\")\n",
    "\n",
    "r = avg_word_length(\"Songs/Midnights (2022)/ts10t8_vigilante_shit.txt\")\n",
    "print(f\"ALBUM 10, TRACK 8: Average word length for 'Vigilante Shit' is {r} characters.\")\n",
    "\n",
    "r = avg_word_length(\"Songs/Midnights (2022)/ts10t9_bejeweled.txt\")\n",
    "print(f\"ALBUM 10, TRACK 9: Average word length for 'Bejeweled' is {r} characters.\")\n",
    "\n",
    "r = avg_word_length(\"Songs/Midnights (2022)/ts10t10_labyrinth.txt\")\n",
    "print(f\"ALBUM 10, TRACK 10: Average word length for 'Labyrinth' is {r} characters.\")\n",
    "\n",
    "r = avg_word_length(\"Songs/Midnights (2022)/ts10t11_karma.txt\")\n",
    "print(f\"ALBUM 10, TRACK 11: Average word length for 'Karma' is {r} characters.\")\n",
    "\n",
    "r = avg_word_length(\"Songs/Midnights (2022)/ts10t12_sweet_nothing.txt\")\n",
    "print(f\"ALBUM 10, TRACK 12: Average word length for 'Sweet Nothing' is {r} characters.\")\n",
    "\n",
    "r = avg_word_length(\"Songs/Midnights (2022)/ts10t13_mastermind.txt\")\n",
    "print(f\"ALBUM 10, TRACK 13: Average word length for 'Mastermind' is {r} characters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81416f69",
   "metadata": {},
   "source": [
    "## 1.3 - Total number of words for each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e231ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALBUM 1, TRACK 1: Word count for \"Tim McGraw\" : 366\n",
      "ALBUM 1, TRACK 2: Word count for \"Picture to Burn\" : 290\n",
      "ALBUM 1, TRACK 3: Word count for \"Teardrops On My Guitar\" : 288\n",
      "ALBUM 1, TRACK 4: Word count for \"A Place In This World\" : 233\n",
      "ALBUM 1, TRACK 5: Word count for \"Cold As You\" : 245\n",
      "ALBUM 1, TRACK 6: Word count for \"The Outside\" : 237\n",
      "ALBUM 1, TRACK 7: Word count for \"Tied Together With a Smile\" : 246\n",
      "ALBUM 1, TRACK 8: Word count for \"Stay Beautiful\" : 267\n",
      "ALBUM 1, TRACK 9: Word count for \"Should've Said No\" : 350\n",
      "ALBUM 1, TRACK 10: Word count for \"Mary's Song\" : 330\n",
      "ALBUM 1, TRACK 11: Word count for \"Our Song\" : 370\n",
      "\n",
      "ALBUM 10, TRACK 1: Word count for \"Lavender Haze\" : 265\n",
      "ALBUM 10, TRACK 2: Word count for \"Maroon\" : 350\n",
      "ALBUM 10, TRACK 3: Word count for \"Anti-Hero\" : 332\n",
      "ALBUM 10, TRACK 4: Word count for \"Snow On The Beach\" : 370\n",
      "ALBUM 10, TRACK 5: Word count for \"You're On Your Own, Kid\" : 300\n",
      "ALBUM 10, TRACK 6: Word count for \"Midnight Rain\" : 279\n",
      "ALBUM 10, TRACK 7: Word count for \"Question...?\" : 434\n",
      "ALBUM 10, TRACK 8: Word count for \"Vigilante Shit\" : 261\n",
      "ALBUM 10, TRACK 9: Word count for \"Bejeweled\" : 420\n",
      "ALBUM 10, TRACK 10: Word count for \"Labyrinth\" : 269\n",
      "ALBUM 10, TRACK 11: Word count for \"Karma\" : 379\n",
      "ALBUM 10, TRACK 12: Word count for \"Sweet Nothing\" : 269\n",
      "ALBUM 10, TRACK 13: Word count for \"Mastermind\" : 336\n"
     ]
    }
   ],
   "source": [
    "#TOTAL WORD COUNT PER SONG\n",
    "\n",
    "file = open(\"Songs/Taylor Swift (2006)/ts1t1_tim_mcgraw.txt\")\n",
    "data = file.read()\n",
    "words = data.split()\n",
    "print('ALBUM 1, TRACK 1: Word count for \"Tim McGraw\" :', len(words))\n",
    "\n",
    "file = open(\"Songs/Taylor Swift (2006)/ts1t2_picture_to_burn.txt\")\n",
    "data = file.read()\n",
    "words = data.split()\n",
    "print('ALBUM 1, TRACK 2: Word count for \"Picture to Burn\" :', len(words))\n",
    "\n",
    "file = open(\"Songs/Taylor Swift (2006)/ts1t3_teardrops_on_my_guitar.txt\")\n",
    "data = file.read()\n",
    "words = data.split()\n",
    "print('ALBUM 1, TRACK 3: Word count for \"Teardrops On My Guitar\" :', len(words))\n",
    "\n",
    "file = open(\"Songs/Taylor Swift (2006)/ts1t4_a_place_in_this_world.txt\")\n",
    "data = file.read()\n",
    "words = data.split()\n",
    "print('ALBUM 1, TRACK 4: Word count for \"A Place In This World\" :', len(words))\n",
    "\n",
    "file = open(\"Songs/Taylor Swift (2006)/ts1t5_cold_as_you.txt\")\n",
    "data = file.read()\n",
    "words = data.split()\n",
    "print('ALBUM 1, TRACK 5: Word count for \"Cold As You\" :', len(words))\n",
    "\n",
    "file = open(\"Songs/Taylor Swift (2006)/ts1t6_the_outside.txt\")\n",
    "data = file.read()\n",
    "words = data.split()\n",
    "print('ALBUM 1, TRACK 6: Word count for \"The Outside\" :', len(words))\n",
    "\n",
    "file = open(\"Songs/Taylor Swift (2006)/ts1t7_tied_together_with_a_smile.txt\")\n",
    "data = file.read()\n",
    "words = data.split()\n",
    "print('ALBUM 1, TRACK 7: Word count for \"Tied Together With a Smile\" :', len(words))\n",
    "\n",
    "file = open(\"Songs/Taylor Swift (2006)/ts1t8_stay_beautiful.txt\")\n",
    "data = file.read()\n",
    "words = data.split()\n",
    "print('ALBUM 1, TRACK 8: Word count for \"Stay Beautiful\" :', len(words))\n",
    "\n",
    "file = open(\"Songs/Taylor Swift (2006)/ts1t9_shouldve_said_no.txt\")\n",
    "data = file.read()\n",
    "words = data.split()\n",
    "print('ALBUM 1, TRACK 9: Word count for \"Should\\'ve Said No\" :', len(words))\n",
    "\n",
    "file = open(\"Songs/Taylor Swift (2006)/ts1t10_marys_song.txt\")\n",
    "data = file.read()\n",
    "words = data.split()\n",
    "print('ALBUM 1, TRACK 10: Word count for \"Mary\\'s Song\" :', len(words))\n",
    "\n",
    "file = open(\"Songs/Taylor Swift (2006)/ts1t11_our_song.txt\")\n",
    "data = file.read()\n",
    "words = data.split()\n",
    "print('ALBUM 1, TRACK 11: Word count for \"Our Song\" :', len(words))\n",
    "\n",
    "print (    )\n",
    "\n",
    "file = open(\"Songs/Midnights (2022)/ts10t1_lavender_haze.txt\")\n",
    "data = file.read()\n",
    "words = data.split()\n",
    "print('ALBUM 10, TRACK 1: Word count for \"Lavender Haze\" :', len(words))\n",
    "\n",
    "file = open(\"Songs/Midnights (2022)/ts10t2_maroon.txt\")\n",
    "data = file.read()\n",
    "words = data.split()\n",
    "print('ALBUM 10, TRACK 2: Word count for \"Maroon\" :', len(words))\n",
    "\n",
    "file = open(\"Songs/Midnights (2022)/ts10t3_anti_hero.txt\")\n",
    "data = file.read()\n",
    "words = data.split()\n",
    "print('ALBUM 10, TRACK 3: Word count for \"Anti-Hero\" :', len(words))\n",
    "\n",
    "file = open(\"Songs/Midnights (2022)/ts10t4_snow_on_the_beach.txt\")\n",
    "data = file.read()\n",
    "words = data.split()\n",
    "print('ALBUM 10, TRACK 4: Word count for \"Snow On The Beach\" :', len(words))\n",
    "\n",
    "file = open(\"Songs/Midnights (2022)/ts10t5_youre_on_your_own_kid.txt\")\n",
    "data = file.read()\n",
    "words = data.split()\n",
    "print('ALBUM 10, TRACK 5: Word count for \"You\\'re On Your Own, Kid\" :', len(words))\n",
    "\n",
    "file = open(\"Songs/Midnights (2022)/ts10t6_midnight_rain.txt\")\n",
    "data = file.read()\n",
    "words = data.split()\n",
    "print('ALBUM 10, TRACK 6: Word count for \"Midnight Rain\" :', len(words))\n",
    "\n",
    "file = open(\"Songs/Midnights (2022)/ts10t7_question.txt\")\n",
    "data = file.read()\n",
    "words = data.split()\n",
    "print('ALBUM 10, TRACK 7: Word count for \"Question...?\" :', len(words))\n",
    "\n",
    "file = open(\"Songs/Midnights (2022)/ts10t8_vigilante_shit.txt\")\n",
    "data = file.read()\n",
    "words = data.split()\n",
    "print('ALBUM 10, TRACK 8: Word count for \"Vigilante Shit\" :', len(words))\n",
    "\n",
    "file = open(\"Songs/Midnights (2022)/ts10t9_bejeweled.txt\")\n",
    "data = file.read()\n",
    "words = data.split()\n",
    "print('ALBUM 10, TRACK 9: Word count for \"Bejeweled\" :', len(words))\n",
    "\n",
    "file = open(\"Songs/Midnights (2022)/ts10t10_labyrinth.txt\")\n",
    "data = file.read()\n",
    "words = data.split()\n",
    "print('ALBUM 10, TRACK 10: Word count for \"Labyrinth\" :', len(words))\n",
    "\n",
    "file = open(\"Songs/Midnights (2022)/ts10t11_karma.txt\")\n",
    "data = file.read()\n",
    "words = data.split()\n",
    "print('ALBUM 10, TRACK 11: Word count for \"Karma\" :', len(words))\n",
    "\n",
    "file = open(\"Songs/Midnights (2022)/ts10t12_sweet_nothing.txt\")\n",
    "data = file.read()\n",
    "words = data.split()\n",
    "print('ALBUM 10, TRACK 12: Word count for \"Sweet Nothing\" :', len(words))\n",
    "\n",
    "file = open(\"Songs/Midnights (2022)/ts10t13_mastermind.txt\")\n",
    "data = file.read()\n",
    "words = data.split()\n",
    "print('ALBUM 10, TRACK 13: Word count for \"Mastermind\" :', len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79778d7a",
   "metadata": {},
   "source": [
    "## 1.4 - Top 10 most frequently used words (excluding function words) for each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "715c785f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALBUM 1, TRACK 1: Top 10 Non-Function Words in 'Tim McGraw':\n",
      "[('think', 22), ('hope', 10), ('tim', 6), ('mcgraw', 6), ('blue', 5), ('night', 5), ('little', 4), ('back', 4), ('long', 3), ('favorite', 3)].\n",
      "ALBUM 1, TRACK 2: Top 10 Non-Function Words in 'Picture to Burn':\n",
      "[(\"you're\", 8), ('burn', 6), (\"i'm\", 5), ('really', 5), ('bad', 5), ('time', 5), ('just', 5), ('hate', 4), ('stupid', 4), ('old', 4)].\n",
      "ALBUM 1, TRACK 3: Top 10 Non-Function Words in 'Teardrops On My Guitar':\n",
      "[(\"he's\", 12), ('drew', 4), ('me,', 4), ('wishing', 4), ('know', 4), ('see', 3), (\"she's\", 3), ('got', 3), (\"'cause\", 3), ('reason', 3)].\n",
      "ALBUM 1, TRACK 4: Top 10 Non-Function Words in 'A Place In This World':\n",
      "[(\"i'm\", 16), ('oh,', 12), ('just', 9), (\"i'll\", 6), ('girl', 6), ('trying', 5), (\"that's\", 5), ('know', 4), ('alone,', 4), ('own', 4)].\n",
      "ALBUM 1, TRACK 5: Top 10 Non-Function Words in 'Cold As You':\n",
      "[(\"i'm\", 4), ('oh,', 4), ('away', 4), ('shame', 3), ('rainy', 3), ('ending', 3), ('given', 3), ('perfect', 3), ('day', 3), (\"sittin'\", 3)].\n",
      "ALBUM 1, TRACK 6: Top 10 Non-Function Words in 'The Outside':\n",
      "[(\"i've\", 7), ('see', 4), ('try', 3), ('better?', 3), (\"let's\", 3), ('you,', 3), (\"ain't\", 3), ('best', 3), ('view', 3), ('looking', 3)].\n",
      "ALBUM 1, TRACK 7: Top 10 Non-Function Words in 'Tied Together With a Smile':\n",
      "[(\"you're\", 16), ('tied', 4), ('smile', 4), ('coming', 4), (\"it's\", 4), (\"'cause\", 3), ('hold', 3), ('on,', 3), ('baby;', 3), ('losing', 3)].\n",
      "ALBUM 1, TRACK 8: Top 10 Non-Function Words in 'Stay Beautiful':\n",
      "[('beautiful', 8), ('oh,', 8), (\"you're\", 6), (\"don't\", 5), ('stay', 5), ('know?', 4), ('beautiful,', 4), ('na,', 4), ('little', 3), ('piece,', 3)].\n",
      "ALBUM 1, TRACK 9: Top 10 Non-Function Words in 'Should've Said No':\n",
      "[(\"should've\", 18), ('back', 10), ('said,', 8), ('\"no\"', 6), (\"shouldn't\", 6), ('no,', 5), ('gone', 4), ('baby,', 4), ('say', 3), ('home', 3)].\n",
      "ALBUM 1, TRACK 10: Top 10 Non-Function Words in 'Mary's Song':\n",
      "[('my,', 12), ('take', 6), ('back', 5), ('said,', 4), ('like', 3), ('two', 3), ('mamas', 3), ('eyes', 3), ('oh', 3), ('\"i', 2)].\n",
      "ALBUM 1, TRACK 11: Top 10 Non-Function Words in 'Our Song':\n",
      "[('song', 8), ('late,', 6), ('got', 5), (\"don't\", 4), (\"'cause\", 4), ('way', 4), (\"didn't\", 4), ('play', 4), ('front', 3), ('slamming', 3)].\n",
      "\n",
      "ALBUM 10, TRACK 1: Top 10 Non-Function Words in 'Lavender Haze':\n",
      "[('lavender', 9), ('haze', 8), ('just', 7), ('get', 6), ('wanna', 5), ('stay', 5), ('say', 4), ('shit', 4), (\"i'm\", 4), ('talk', 4)].\n",
      "ALBUM 10, TRACK 2: Top 10 Non-Function Words in 'Maroon':\n",
      "[('maroon', 8), ('scarlet,', 6), (\"that's\", 4), ('burgundy', 3), ('t-shirt', 3), ('splashed', 3), ('wine', 3), ('blood', 3), ('rushed', 3), ('cheeks', 3)].\n",
      "ALBUM 10, TRACK 3: Top 10 Non-Function Words in 'Anti-Hero':\n",
      "[(\"it's\", 13), ('problem,', 7), ('hi', 6), (\"i'm\", 6), ('everybody', 6), (\"i'll\", 5), ('agrees', 5), ('time)', 4), ('teatime', 3), ('stare', 3)].\n",
      "ALBUM 10, TRACK 4: Top 10 Non-Function Words in 'Snow On The Beach':\n",
      "[(\"it's\", 24), ('like', 21), ('snow', 15), ('down,', 15), ('coming', 15), ('beach', 14), ('flying', 4), ('feels', 4), (\"comin'\", 4), ('sound,', 4)].\n",
      "ALBUM 10, TRACK 5: Top 10 Non-Function Words in 'You're On Your Own, Kid':\n",
      "[(\"you're\", 5), ('own,', 4), ('kid', 4), ('take', 4), ('just', 3), ('sprinkler', 3), ('splashes', 3), ('fireplace', 3), ('ashes', 3), ('play', 2)].\n",
      "ALBUM 10, TRACK 6: Top 10 Non-Function Words in 'Midnight Rain':\n",
      "[('wanted', 13), ('like', 7), ('midnight', 6), ('comfortable,', 4), ('pain', 4), ('bride,', 4), ('making', 4), ('own', 4), ('name', 4), ('chasing', 4)].\n",
      "ALBUM 10, TRACK 7: Top 10 Non-Function Words in 'Question...?':\n",
      "[('just', 6), ('oh', 6), ('wish', 6), ('ask', 4), ('crowded', 4), ('making', 4), ('fun', 4), ('her?', 4), ('question?', 3), ('kiss', 3)].\n",
      "ALBUM 10, TRACK 8: Top 10 Non-Function Words in 'Vigilante Shit':\n",
      "[(\"don't\", 12), ('dress', 6), ('get', 6), ('lately', 5), ('revenge', 5), ('dressing', 4), (\"i've\", 3), ('start', 3), ('shit,', 3), ('tell', 3)].\n",
      "ALBUM 10, TRACK 9: Top 10 Non-Function Words in 'Bejeweled':\n",
      "[('polish', 7), (\"don't\", 6), ('real', 6), (\"i'm\", 5), ('girl', 5), ('best', 4), ('believe', 4), ('bejeweled', 4), ('walk', 4), ('room', 4)].\n",
      "ALBUM 10, TRACK 10: Top 10 Non-Function Words in 'Labyrinth':\n",
      "[('love', 17), (\"i'm\", 15), ('falling', 15), ('oh,', 10), ('right', 7), ('break', 6), ('(falling', 6), ('uh-oh,', 5), ('no,', 5), ('thought', 5)].\n",
      "ALBUM 10, TRACK 11: Top 10 Non-Function Words in 'Karma':\n",
      "[('karma', 23), ('like', 11), (\"'cause\", 8), (\"it's\", 6), ('boyfriend', 4), ('god', 4), (\"karma's\", 4), ('relaxing', 4), ('thought', 4), ('sweet', 4)].\n",
      "ALBUM 10, TRACK 12: Top 10 Non-Function Words in 'Sweet Nothing':\n",
      "[('sweet', 7), ('ooh', 5), ('said', 5), ('end', 5), ('home', 5), (\"they're\", 5), ('push', 5), ('kitchen', 5), ('coming', 4), (\"everyone's\", 4)].\n",
      "ALBUM 10, TRACK 13: Top 10 Non-Function Words in 'Mastermind':\n",
      "[(\"i'm\", 8), ('mastermind', 6), ('told', 5), (\"'cause\", 5), ('time', 4), ('saw', 4), ('accidental', 3), ('night', 3), ('laid', 3), ('groundwork', 3)].\n"
     ]
    }
   ],
   "source": [
    "#TOP10 MOST USED WORDS\n",
    "\n",
    "file = open(\"function_words.txt\")\n",
    "data = file.read()\n",
    "function_words = data.split()\n",
    "\n",
    "def get_most_freq_word(a):\n",
    "    fhandle = open(a)\n",
    "    s = fhandle.read()\n",
    "    words = s.lower().split()\n",
    "    d = dict()\n",
    "    for word in words:\n",
    "        if word in function_words:\n",
    "            continue\n",
    "        if word not in d:\n",
    "            d[word] = 1\n",
    "        else:\n",
    "            d[word] += 1\n",
    "    s = sorted(d.items(), key=lambda item:-item[1])\n",
    "    return s[:10]\n",
    "\n",
    "f = get_most_freq_word(\"Songs/Taylor Swift (2006)/ts1t1_tim_mcgraw.txt\")\n",
    "print(f\"ALBUM 1, TRACK 1: Top 10 Non-Function Words in 'Tim McGraw':\\n{f}.\")\n",
    "\n",
    "f = get_most_freq_word(\"Songs/Taylor Swift (2006)/ts1t2_picture_to_burn.txt\")\n",
    "print(f\"ALBUM 1, TRACK 2: Top 10 Non-Function Words in 'Picture to Burn':\\n{f}.\")\n",
    "\n",
    "f = get_most_freq_word(\"Songs/Taylor Swift (2006)/ts1t3_teardrops_on_my_guitar.txt\")\n",
    "print(f\"ALBUM 1, TRACK 3: Top 10 Non-Function Words in 'Teardrops On My Guitar':\\n{f}.\")\n",
    "\n",
    "f = get_most_freq_word(\"Songs/Taylor Swift (2006)/ts1t4_a_place_in_this_world.txt\")\n",
    "print(f\"ALBUM 1, TRACK 4: Top 10 Non-Function Words in 'A Place In This World':\\n{f}.\")\n",
    "\n",
    "f = get_most_freq_word(\"Songs/Taylor Swift (2006)/ts1t5_cold_as_you.txt\")\n",
    "print(f\"ALBUM 1, TRACK 5: Top 10 Non-Function Words in 'Cold As You':\\n{f}.\")\n",
    "\n",
    "f = get_most_freq_word(\"Songs/Taylor Swift (2006)/ts1t6_the_outside.txt\")\n",
    "print(f\"ALBUM 1, TRACK 6: Top 10 Non-Function Words in 'The Outside':\\n{f}.\")\n",
    "\n",
    "f = get_most_freq_word(\"Songs/Taylor Swift (2006)/ts1t7_tied_together_with_a_smile.txt\")\n",
    "print(f\"ALBUM 1, TRACK 7: Top 10 Non-Function Words in 'Tied Together With a Smile':\\n{f}.\")\n",
    "\n",
    "f = get_most_freq_word(\"Songs/Taylor Swift (2006)/ts1t8_stay_beautiful.txt\")\n",
    "print(f\"ALBUM 1, TRACK 8: Top 10 Non-Function Words in 'Stay Beautiful':\\n{f}.\")\n",
    "\n",
    "f = get_most_freq_word(\"Songs/Taylor Swift (2006)/ts1t9_shouldve_said_no.txt\")\n",
    "print(f\"ALBUM 1, TRACK 9: Top 10 Non-Function Words in 'Should've Said No':\\n{f}.\")\n",
    "\n",
    "f = get_most_freq_word(\"Songs/Taylor Swift (2006)/ts1t10_marys_song.txt\")\n",
    "print(f\"ALBUM 1, TRACK 10: Top 10 Non-Function Words in 'Mary's Song':\\n{f}.\")\n",
    "\n",
    "f = get_most_freq_word(\"Songs/Taylor Swift (2006)/ts1t11_our_song.txt\")\n",
    "print(f\"ALBUM 1, TRACK 11: Top 10 Non-Function Words in 'Our Song':\\n{f}.\")\n",
    "print(    )\n",
    "\n",
    "f = get_most_freq_word(\"Songs/Midnights (2022)/ts10t1_lavender_haze.txt\")\n",
    "print(f\"ALBUM 10, TRACK 1: Top 10 Non-Function Words in 'Lavender Haze':\\n{f}.\")\n",
    "\n",
    "f = get_most_freq_word(\"Songs/Midnights (2022)/ts10t2_maroon.txt\")\n",
    "print(f\"ALBUM 10, TRACK 2: Top 10 Non-Function Words in 'Maroon':\\n{f}.\")\n",
    "\n",
    "f = get_most_freq_word(\"Songs/Midnights (2022)/ts10t3_anti_hero.txt\")\n",
    "print(f\"ALBUM 10, TRACK 3: Top 10 Non-Function Words in 'Anti-Hero':\\n{f}.\")\n",
    "\n",
    "f = get_most_freq_word(\"Songs/Midnights (2022)/ts10t4_snow_on_the_beach.txt\")\n",
    "print(f\"ALBUM 10, TRACK 4: Top 10 Non-Function Words in 'Snow On The Beach':\\n{f}.\")\n",
    "\n",
    "f = get_most_freq_word(\"Songs/Midnights (2022)/ts10t5_youre_on_your_own_kid.txt\")\n",
    "print(f\"ALBUM 10, TRACK 5: Top 10 Non-Function Words in 'You're On Your Own, Kid':\\n{f}.\")\n",
    "\n",
    "f = get_most_freq_word(\"Songs/Midnights (2022)/ts10t6_midnight_rain.txt\")\n",
    "print(f\"ALBUM 10, TRACK 6: Top 10 Non-Function Words in 'Midnight Rain':\\n{f}.\")\n",
    "\n",
    "f = get_most_freq_word(\"Songs/Midnights (2022)/ts10t7_question.txt\")\n",
    "print(f\"ALBUM 10, TRACK 7: Top 10 Non-Function Words in 'Question...?':\\n{f}.\")\n",
    "\n",
    "f = get_most_freq_word(\"Songs/Midnights (2022)/ts10t8_vigilante_shit.txt\")\n",
    "print(f\"ALBUM 10, TRACK 8: Top 10 Non-Function Words in 'Vigilante Shit':\\n{f}.\")\n",
    "\n",
    "f = get_most_freq_word(\"Songs/Midnights (2022)/ts10t9_bejeweled.txt\")\n",
    "print(f\"ALBUM 10, TRACK 9: Top 10 Non-Function Words in 'Bejeweled':\\n{f}.\")\n",
    "\n",
    "f = get_most_freq_word(\"Songs/Midnights (2022)/ts10t10_labyrinth.txt\")\n",
    "print(f\"ALBUM 10, TRACK 10: Top 10 Non-Function Words in 'Labyrinth':\\n{f}.\")\n",
    "\n",
    "f = get_most_freq_word(\"Songs/Midnights (2022)/ts10t11_karma.txt\")\n",
    "print(f\"ALBUM 10, TRACK 11: Top 10 Non-Function Words in 'Karma':\\n{f}.\")\n",
    "\n",
    "f = get_most_freq_word(\"Songs/Midnights (2022)/ts10t12_sweet_nothing.txt\")\n",
    "print(f\"ALBUM 10, TRACK 12: Top 10 Non-Function Words in 'Sweet Nothing':\\n{f}.\")\n",
    "\n",
    "f = get_most_freq_word(\"Songs/Midnights (2022)/ts10t13_mastermind.txt\")\n",
    "print(f\"ALBUM 10, TRACK 13: Top 10 Non-Function Words in 'Mastermind':\\n{f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b7078b",
   "metadata": {},
   "source": [
    "## 1.5 - Top 10 most frequently used bigrams, and trigrams (including function words) for each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a18bdd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALBUM 1, TRACK 1: Top 10 Bigrams for 'Tim McGraw': [(18, ('you', 'think')), (10, ('i', 'hope')), (9, ('when', 'you')), (9, ('hope', 'you')), (7, ('think', 'of')), (6, ('tim', 'mcgraw')), (6, ('think', 'tim')), (6, ('mcgraw', 'i')), (5, ('on', 'your')), (4, ('of', 'me'))]\n",
      "ALBUM 1, TRACK 2: Top 10 Bigrams for 'Picture to Burn': [(4, (\"you're\", 'a')), (4, ('you', 'never')), (4, (\"who's\", 'really')), (4, ('watch', 'me')), (4, ('wasted', 'time')), (4, ('truck', 'you')), (4, ('to', 'burn')), (4, ('that', 'stupid')), (4, ('stupid', 'old')), (4, ('strike', 'a'))]\n",
      "ALBUM 1, TRACK 3: Top 10 Bigrams for 'Teardrops On My Guitar': [(7, (\"he's\", 'the')), (4, ('that', 'i')), (3, ('why', 'i')), (3, ('the', 'teardrops')), (3, ('the', 'song')), (3, ('the', 'reason')), (3, ('the', 'only')), (3, ('the', 'car')), (3, ('teardrops', 'on')), (3, ('song', 'in'))]\n",
      "ALBUM 1, TRACK 4: Top 10 Bigrams for 'A Place In This World': [(8, ('just', 'a')), (8, (\"i'm\", 'just')), (7, ('oh', \"i'm\")), (7, ('a', 'girl')), (6, ('on', 'my')), (6, (\"i'll\", 'be')), (5, ('trying', 'to')), (4, (\"that's\", 'all')), (4, ('own', 'and')), (4, ('my', 'own'))]\n",
      "ALBUM 1, TRACK 5: Top 10 Bigrams for 'Cold As You': [(6, ('what', 'a')), (3, ('to', 'a')), (3, ('through', \"i've\")), (3, (\"thinkin'\", 'it')), (3, ('that', \"i'm\")), (3, (\"sittin'\", 'here')), (3, ('shame', 'what')), (3, ('rainy', 'ending')), (3, ('perfect', 'day')), (3, ('oh', 'what'))]\n",
      "ALBUM 1, TRACK 6: Top 10 Bigrams for 'The Outside': [(6, ('the', 'outside')), (6, ('on', 'the')), (6, ('oh', 'oh')), (4, ('to', 'be')), (4, ('see', 'you')), (3, ('you', 'this')), (3, ('view', 'on')), (3, ('try', 'to')), (3, ('this', \"ain't\")), (3, ('the', 'best'))]\n",
      "ALBUM 1, TRACK 7: Top 10 Bigrams for 'Tied Together With a Smile': [(6, ('that', 'you')), (5, ('with', 'a')), (5, ('a', 'smile')), (4, (\"you're\", 'tied')), (4, (\"you're\", 'coming')), (4, ('together', 'with')), (4, ('tied', 'together')), (4, ('smile', 'but')), (4, ('coming', 'undone')), (4, ('but', \"you're\"))]\n",
      "ALBUM 1, TRACK 8: Top 10 Bigrams for 'Stay Beautiful': [(6, ('oh', 'oh')), (5, ('stay', 'beautiful')), (5, ('beautiful', 'beautiful')), (4, ('oh', 'but')), (4, ('na', 'na')), (4, ('it', \"don't\")), (4, ('if', 'it')), (4, ('but', 'if')), (3, ('your', 'love')), (3, (\"you've\", 'looked'))]\n",
      "ALBUM 1, TRACK 9: Top 10 Bigrams for 'Should've Said No': [(15, ('you', \"should've\")), (6, ('to', 'me')), (6, (\"shouldn't\", 'be')), (6, (\"should've\", 'said')), (6, ('said', 'no')), (6, ('get', 'back')), (6, ('back', 'to')), (5, ('no', 'no')), (5, ('and', 'you')), (4, ('no', 'you'))]\n",
      "ALBUM 1, TRACK 10: Top 10 Bigrams for 'Mary's Song': [(12, ('my', 'my')), (6, ('take', 'me')), (5, ('oh', 'my')), (5, ('me', 'back')), (5, ('and', 'our')), (4, ('to', 'the')), (4, ('back', 'to')), (3, ('used', 'to')), (3, ('our', 'mamas')), (3, ('in', 'the'))]\n",
      "ALBUM 1, TRACK 11: Top 10 Bigrams for 'Our Song': [(8, ('our', 'song')), (6, ('song', 'is')), (6, ('is', 'the')), (4, ('play', 'it')), (4, ('on', 'the')), (4, ('it', 'again')), (4, ('i', 'was')), (4, ('and', 'i')), (3, ('window', 'when')), (3, ('when', \"we're\"))]\n",
      "\n",
      "ALBUM 10, TRACK 1: Top 10 Bigrams for 'Lavender Haze': [(9, ('lavender', 'haze')), (7, ('i', 'just')), (6, ('that', 'lavender')), (6, ('it', 'off')), (6, ('get', 'it')), (5, ('wanna', 'stay')), (5, ('just', 'wanna')), (4, ('stay', 'in')), (4, ('me', 'i')), (4, ('in', 'that'))]\n",
      "ALBUM 10, TRACK 2: Top 10 Bigrams for 'Maroon': [(11, ('it', 'was')), (9, ('was', 'maroon')), (6, ('so', 'scarlet')), (6, ('scarlet', 'it')), (6, ('on', 'my')), (4, ('how', 'the')), (4, ('and', 'i')), (3, ('your', 'wine')), (3, ('you', 'splashed')), (3, ('with', 'your'))]\n",
      "ALBUM 10, TRACK 3: Top 10 Bigrams for 'Anti-Hero': [(13, (\"it's\", 'me')), (7, ('the', 'problem')), (7, ('problem', \"it's\")), (7, (\"i'm\", 'the')), (6, ('me', 'hi')), (6, ('for', 'the')), (6, ('everybody', 'agrees')), (5, ('in', 'the')), (5, ('hi', \"i'm\")), (4, ('but', 'never'))]\n",
      "ALBUM 10, TRACK 4: Top 10 Bigrams for 'Snow On The Beach': [(17, ('like', 'snow')), (15, (\"it's\", 'coming')), (15, ('coming', 'down')), (14, ('the', 'beach')), (13, ('down', \"it's\")), (11, ('snow', 'on')), (11, ('on', 'the')), (9, ('beach', 'like')), (4, ('sound', \"it's\")), (4, ('no', 'sound'))]\n",
      "ALBUM 10, TRACK 5: Top 10 Bigrams for 'You're On Your Own, Kid': [(4, ('your', 'own')), (4, (\"you're\", 'on')), (4, ('own', 'kid')), (4, ('on', 'your')), (3, ('you', 'always')), (3, ('to', 'fireplace')), (3, ('sprinkler', 'splashes')), (3, ('splashes', 'to')), (3, ('kid', 'you')), (3, ('have', 'been'))]\n",
      "ALBUM 10, TRACK 6: Top 10 Bigrams for 'Midnight Rain': [(8, ('he', 'wanted')), (6, ('i', 'was')), (5, ('of', 'me')), (4, ('was', 'making')), (4, ('wanted', 'that')), (4, ('wanted', 'it')), (4, ('wanted', 'a')), (4, ('the', 'same')), (4, ('that', 'pain')), (4, ('that', 'fame'))]\n",
      "ALBUM 10, TRACK 7: Top 10 Bigrams for 'Question...?': [(13, ('did', 'you')), (7, ('a', 'question')), (6, ('you', 'wish')), (5, ('it', 'was')), (4, ('you', 'a')), (4, ('was', 'making')), (4, ('of', 'you')), (4, ('of', 'a')), (4, ('more', 'of')), (4, ('making', 'fun'))]\n",
      "ALBUM 10, TRACK 8: Top 10 Bigrams for 'Vigilante Shit': [(7, ('i', \"don't\")), (6, ('dress', 'for')), (6, (\"don't\", 'dress')), (5, ('for', 'revenge')), (4, ('on', 'the')), (4, ('dressing', 'for')), (4, ('been', 'dressing')), (3, ('you', 'how')), (3, ('the', 'weekends')), (3, ('tell', 'you'))]\n",
      "ALBUM 10, TRACK 9: Top 10 Bigrams for 'Bejeweled': [(10, ('when', 'i')), (8, ('in', 'the')), (7, ('polish', 'up')), (7, ('i', 'polish')), (6, ('up', 'real')), (5, ('i', 'can')), (5, ('a', 'girl')), (4, ('whole', 'place')), (4, ('walk', 'in')), (4, ('the', 'whole'))]\n",
      "ALBUM 10, TRACK 10: Top 10 Bigrams for 'Labyrinth': [(21, ('in', 'love')), (21, ('falling', 'in')), (15, (\"i'm\", 'falling')), (10, ('oh', \"i'm\")), (7, ('love', 'again')), (5, ('you', 'turn')), (5, ('was', 'going')), (5, ('uh', 'oh')), (5, ('turn', 'it')), (5, ('thought', 'the'))]\n",
      "ALBUM 10, TRACK 11: Top 10 Bigrams for 'Karma': [(20, ('karma', 'is')), (9, ('is', 'a')), (6, ('in', 'my')), (5, ('my', 'boyfriend')), (5, ('is', 'the')), (5, ('is', 'my')), (5, ('boyfriend', 'karma')), (4, ('what', 'i')), (4, ('sweet', 'like')), (4, ('relaxing', 'thought'))]\n",
      "ALBUM 10, TRACK 12: Top 10 Bigrams for 'Sweet Nothing': [(5, (\"you're\", 'in')), (5, ('up', 'to')), (5, ('to', 'something')), (5, (\"they're\", 'push')), (5, ('they', 'said')), (5, ('the', 'kitchen')), (5, ('the', 'end')), (5, ('said', 'the')), (5, ('push', 'and')), (5, ('outside', \"they're\"))]\n",
      "ALBUM 10, TRACK 13: Top 10 Bigrams for 'Mastermind': [(6, (\"i'm\", 'a')), (6, ('and', 'the')), (6, ('a', 'mastermind')), (5, ('told', 'you')), (5, ('it', 'was')), (5, ('i', 'told')), (4, ('what', 'if')), (4, ('the', 'first')), (4, ('if', 'i')), (3, (\"you're\", 'mine'))]\n"
     ]
    }
   ],
   "source": [
    "#TOP 10 BIGRAMS FOR EACH SONG\n",
    "\n",
    "import string\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "from nltk.tokenize import sent_tokenize  \n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(\"['\\w']+\")\n",
    "\n",
    "def bigram_list(file):    \n",
    "    fin = open(file) \n",
    "    s = fin.read()\n",
    "    \n",
    "    list_of_bigrams = []\n",
    "    for sent in sent_tokenize(s):\n",
    "        sent = sent.lower()\n",
    "        tokens = tokenizer.tokenize(sent)\n",
    "        res = list(nltk.bigrams(tokens))\n",
    "        list_of_bigrams.extend(res)\n",
    "\n",
    "    d = dict()\n",
    "    for t in list_of_bigrams:\n",
    "        if t not in d:\n",
    "            d[t] = 1\n",
    "        else:\n",
    "            d[t] += 1\n",
    "    lst = []\n",
    "    for key, val in d.items():\n",
    "        t = (val, key)\n",
    "        lst.append(t)\n",
    "    lst.sort(reverse=True)\n",
    "    return lst[:10]\n",
    " \n",
    "b = bigram_list(\"Songs/Taylor Swift (2006)/ts1t1_tim_mcgraw.txt\")\n",
    "print(f\"ALBUM 1, TRACK 1: Top 10 Bigrams for 'Tim McGraw': {b}\")\n",
    "\n",
    "b = bigram_list(\"Songs/Taylor Swift (2006)/ts1t2_picture_to_burn.txt\")\n",
    "print(f\"ALBUM 1, TRACK 2: Top 10 Bigrams for 'Picture to Burn': {b}\")\n",
    "\n",
    "b = bigram_list(\"Songs/Taylor Swift (2006)/ts1t3_teardrops_on_my_guitar.txt\")\n",
    "print(f\"ALBUM 1, TRACK 3: Top 10 Bigrams for 'Teardrops On My Guitar': {b}\")\n",
    "\n",
    "b = bigram_list(\"Songs/Taylor Swift (2006)/ts1t4_a_place_in_this_world.txt\")\n",
    "print(f\"ALBUM 1, TRACK 4: Top 10 Bigrams for 'A Place In This World': {b}\")\n",
    "\n",
    "b = bigram_list(\"Songs/Taylor Swift (2006)/ts1t5_cold_as_you.txt\")\n",
    "print(f\"ALBUM 1, TRACK 5: Top 10 Bigrams for 'Cold As You': {b}\")\n",
    "\n",
    "b = bigram_list(\"Songs/Taylor Swift (2006)/ts1t6_the_outside.txt\")\n",
    "print(f\"ALBUM 1, TRACK 6: Top 10 Bigrams for 'The Outside': {b}\")\n",
    "\n",
    "b = bigram_list(\"Songs/Taylor Swift (2006)/ts1t7_tied_together_with_a_smile.txt\")\n",
    "print(f\"ALBUM 1, TRACK 7: Top 10 Bigrams for 'Tied Together With a Smile': {b}\")\n",
    "\n",
    "b = bigram_list(\"Songs/Taylor Swift (2006)/ts1t8_stay_beautiful.txt\")\n",
    "print(f\"ALBUM 1, TRACK 8: Top 10 Bigrams for 'Stay Beautiful': {b}\")\n",
    "\n",
    "b = bigram_list(\"Songs/Taylor Swift (2006)/ts1t9_shouldve_said_no.txt\")\n",
    "print(f\"ALBUM 1, TRACK 9: Top 10 Bigrams for 'Should've Said No': {b}\")\n",
    "\n",
    "b = bigram_list(\"Songs/Taylor Swift (2006)/ts1t10_marys_song.txt\")\n",
    "print(f\"ALBUM 1, TRACK 10: Top 10 Bigrams for 'Mary's Song': {b}\")\n",
    "\n",
    "b = bigram_list(\"Songs/Taylor Swift (2006)/ts1t11_our_song.txt\")\n",
    "print(f\"ALBUM 1, TRACK 11: Top 10 Bigrams for 'Our Song': {b}\")\n",
    "\n",
    "print(   )\n",
    "\n",
    "b = bigram_list(\"Songs/Midnights (2022)/ts10t1_lavender_haze.txt\")\n",
    "print(f\"ALBUM 10, TRACK 1: Top 10 Bigrams for 'Lavender Haze': {lst[:10]}\")\n",
    "\n",
    "b = bigram_list(\"Songs/Midnights (2022)/ts10t2_maroon.txt\")\n",
    "print(f\"ALBUM 10, TRACK 2: Top 10 Bigrams for 'Maroon': {b}\")\n",
    "\n",
    "b = bigram_list(\"Songs/Midnights (2022)/ts10t3_anti_hero.txt\")\n",
    "print(f\"ALBUM 10, TRACK 3: Top 10 Bigrams for 'Anti-Hero': {b}\")\n",
    "\n",
    "b = bigram_list(\"Songs/Midnights (2022)/ts10t4_snow_on_the_beach.txt\")\n",
    "print(f\"ALBUM 10, TRACK 4: Top 10 Bigrams for 'Snow On The Beach': {b}\")\n",
    "     \n",
    "b = bigram_list(\"Songs/Midnights (2022)/ts10t5_youre_on_your_own_kid.txt\")\n",
    "print(f\"ALBUM 10, TRACK 5: Top 10 Bigrams for 'You're On Your Own, Kid': {b}\")\n",
    "\n",
    "b = bigram_list(\"Songs/Midnights (2022)/ts10t6_midnight_rain.txt\")\n",
    "print(f\"ALBUM 10, TRACK 6: Top 10 Bigrams for 'Midnight Rain': {b}\")\n",
    "\n",
    "b = bigram_list(\"Songs/Midnights (2022)/ts10t7_question.txt\")\n",
    "print(f\"ALBUM 10, TRACK 7: Top 10 Bigrams for 'Question...?': {b}\")\n",
    "\n",
    "b = bigram_list(\"Songs/Midnights (2022)/ts10t8_vigilante_shit.txt\")\n",
    "print(f\"ALBUM 10, TRACK 8: Top 10 Bigrams for 'Vigilante Shit': {b}\")\n",
    "\n",
    "b = bigram_list(\"Songs/Midnights (2022)/ts10t9_bejeweled.txt\")\n",
    "print(f\"ALBUM 10, TRACK 9: Top 10 Bigrams for 'Bejeweled': {b}\")\n",
    "\n",
    "b = bigram_list(\"Songs/Midnights (2022)/ts10t10_labyrinth.txt\")\n",
    "print(f\"ALBUM 10, TRACK 10: Top 10 Bigrams for 'Labyrinth': {b}\")\n",
    "\n",
    "b = bigram_list(\"Songs/Midnights (2022)/ts10t11_karma.txt\")\n",
    "print(f\"ALBUM 10, TRACK 11: Top 10 Bigrams for 'Karma': {b}\")\n",
    "\n",
    "b = bigram_list(\"Songs/Midnights (2022)/ts10t12_sweet_nothing.txt\")\n",
    "print(f\"ALBUM 10, TRACK 12: Top 10 Bigrams for 'Sweet Nothing': {b}\")\n",
    "\n",
    "b = bigram_list(\"Songs/Midnights (2022)/ts10t13_mastermind.txt\")\n",
    "print(f\"ALBUM 10, TRACK 13: Top 10 Bigrams for 'Mastermind': {b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae372b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALBUM 1, TRACK 1: Top 10 Trigrams for 'Tim McGraw': [(9, ('when', 'you', 'think')), (9, ('i', 'hope', 'you')), (9, ('hope', 'you', 'think')), (6, ('you', 'think', 'tim')), (6, ('tim', 'mcgraw', 'i')), (6, ('think', 'tim', 'mcgraw')), (6, ('mcgraw', 'i', 'hope')), (4, ('think', 'of', 'me')), (3, ('your', 'chest', 'and')), (3, ('you', 'think', 'that'))]\n",
      "ALBUM 1, TRACK 2: Top 10 Trigrams for 'Picture to Burn': [(4, (\"you're\", 'a', 'redneck')), (4, ('you', 'never', 'let')), (4, (\"who's\", 'really', 'bad')), (4, ('watch', 'me', 'strike')), (4, ('truck', 'you', 'never')), (4, ('that', 'stupid', 'old')), (4, ('stupid', 'old', 'pickup')), (4, ('strike', 'a', 'match')), (4, ('so', 'watch', 'me')), (4, ('redneck', 'heartbreak', \"who's\"))]\n",
      "ALBUM 1, TRACK 3: Top 10 Trigrams for 'Teardrops On My Guitar': [(3, ('why', 'i', 'do')), (3, ('the', 'teardrops', 'on')), (3, ('the', 'song', 'in')), (3, ('the', 'reason', 'for')), (3, ('the', 'car', 'i')), (3, ('teardrops', 'on', 'my')), (3, ('song', 'in', 'the')), (3, ('singing', \"don't\", 'know')), (3, ('reason', 'for', 'the')), (3, ('on', 'my', 'guitar'))]\n",
      "ALBUM 1, TRACK 4: Top 10 Trigrams for 'A Place In This World': [(7, ('just', 'a', 'girl')), (7, (\"i'm\", 'just', 'a')), (6, ('oh', \"i'm\", 'just')), (4, (\"that's\", 'all', 'i')), (4, ('own', 'and', \"that's\")), (4, ('on', 'my', 'own')), (4, ('my', 'own', 'and')), (4, (\"i'm\", 'alone', 'on')), (4, ('and', \"that's\", 'all')), (4, ('alone', 'on', 'my'))]\n",
      "ALBUM 1, TRACK 5: Top 10 Trigrams for 'Cold As You': [(3, ('what', 'a', 'shame')), (3, ('what', 'a', 'rainy')), (3, ('to', 'a', 'perfect')), (3, ('through', \"i've\", 'never')), (3, (\"thinkin'\", 'it', 'through')), (3, ('that', \"i'm\", \"sittin'\")), (3, (\"sittin'\", 'here', \"thinkin'\")), (3, ('shame', 'what', 'a')), (3, ('rainy', 'ending', 'given')), (3, ('oh', 'what', 'a'))]\n",
      "ALBUM 1, TRACK 6: Top 10 Trigrams for 'The Outside': [(6, ('on', 'the', 'outside')), (5, ('oh', 'oh', 'oh')), (3, ('you', 'this', \"ain't\")), (3, ('view', 'on', 'the')), (3, ('try', 'to', 'be')), (3, ('to', 'be', 'better')), (3, ('this', \"ain't\", 'the')), (3, ('the', 'outside', 'looking')), (3, ('the', 'best', 'view')), (3, ('still', 'see', 'you'))]\n",
      "ALBUM 1, TRACK 7: Top 10 Trigrams for 'Tied Together With a Smile': [(5, ('with', 'a', 'smile')), (4, (\"you're\", 'tied', 'together')), (4, (\"you're\", 'coming', 'undone')), (4, ('together', 'with', 'a')), (4, ('tied', 'together', 'with')), (4, ('smile', 'but', \"you're\")), (4, ('but', \"you're\", 'coming')), (4, ('a', 'smile', 'but')), (3, (\"you're\", 'losing', 'it')), (3, (\"you're\", 'jumping', 'into'))]\n",
      "ALBUM 1, TRACK 8: Top 10 Trigrams for 'Stay Beautiful': [(5, ('oh', 'oh', 'oh')), (4, ('oh', 'but', 'if')), (4, ('if', 'it', \"don't\")), (4, ('but', 'if', 'it')), (4, ('beautiful', 'beautiful', 'beautiful')), (3, ('your', 'love', 'leads')), (3, (\"you've\", 'looked', 'for')), (3, (\"you're\", 'really', 'gonna')), (3, (\"you're\", 'beautiful', 'every')), (3, ('you', 'find', 'everything'))]\n",
      "ALBUM 1, TRACK 9: Top 10 Trigrams for 'Should've Said No': [(6, ('you', \"should've\", 'said')), (6, (\"should've\", 'said', 'no')), (6, ('get', 'back', 'to')), (6, ('back', 'to', 'me')), (4, ('no', 'you', \"should've\")), (4, ('no', 'no', 'no')), (3, ('your', 'mind', 'i')), (3, ('you', \"shouldn't\", 'be')), (3, ('you', \"should've\", 'thought')), (3, ('you', \"should've\", 'known'))]\n",
      "ALBUM 1, TRACK 10: Top 10 Trigrams for 'Mary's Song': [(7, ('my', 'my', 'my')), (5, ('take', 'me', 'back')), (5, ('oh', 'my', 'my')), (4, ('me', 'back', 'to')), (4, ('back', 'to', 'the')), (3, ('and', 'our', 'mamas')), (2, ('you', 'never', 'did')), (2, ('you', 'like', 'the')), (2, ('you', 'and', 'i')), (2, ('used', 'to', 'joke'))]\n",
      "ALBUM 1, TRACK 11: Top 10 Trigrams for 'Our Song': [(6, ('song', 'is', 'the')), (6, ('our', 'song', 'is')), (4, ('play', 'it', 'again')), (3, ('window', 'when', \"we're\")), (3, ('when', \"we're\", 'on')), (3, ('when', 'i', 'got')), (3, (\"we're\", 'on', 'the')), (3, ('the', 'slamming', 'screen')), (3, ('the', 'phone', 'and')), (3, ('the', 'first', 'date'))]\n",
      "\n",
      "ALBUM 10, TRACK 1: Top 10 Trigrams for 'Lavender Haze': [(6, ('that', 'lavender', 'haze')), (6, ('get', 'it', 'off')), (5, ('just', 'wanna', 'stay')), (5, ('i', 'just', 'wanna')), (4, ('wanna', 'stay', 'in')), (4, ('stay', 'in', 'that')), (4, ('in', 'that', 'lavender')), (3, ('your', 'chest', 'get')), (3, ('what', 'people', 'say')), (3, ('want', 'from', 'me'))]\n",
      "ALBUM 10, TRACK 2: Top 10 Trigrams for 'Maroon': [(9, ('it', 'was', 'maroon')), (6, ('so', 'scarlet', 'it')), (6, ('scarlet', 'it', 'was')), (3, ('your', 'wine', 'into')), (3, ('you', 'splashed', 'your')), (3, ('wine', 'into', 'me')), (3, ('when', 'you', 'splashed')), (3, ('was', 'maroon', 'the')), (3, ('used', 'to', 'call')), (3, ('to', 'call', 'home'))]\n",
      "ALBUM 10, TRACK 3: Top 10 Trigrams for 'Anti_Hero': [(7, ('the', 'problem', \"it's\")), (7, ('problem', \"it's\", 'me')), (7, (\"i'm\", 'the', 'problem')), (6, (\"it's\", 'me', 'hi')), (5, ('hi', \"i'm\", 'the')), (4, ('me', 'hi', \"i'm\")), (3, ('the', 'sun', 'but')), (3, ('the', 'mirror', 'it')), (3, ('sun', 'but', 'never')), (3, ('stare', 'directly', 'at'))]\n",
      "ALBUM 10, TRACK 4: Top 10 Trigrams for 'Snow On The Beach': [(15, (\"it's\", 'coming', 'down')), (13, ('down', \"it's\", 'coming')), (12, ('coming', 'down', \"it's\")), (11, ('snow', 'on', 'the')), (11, ('on', 'the', 'beach')), (11, ('like', 'snow', 'on')), (9, ('the', 'beach', 'like')), (9, ('beach', 'like', 'snow')), (4, ('sound', \"it's\", 'all')), (4, ('no', 'sound', \"it's\"))]\n",
      "ALBUM 10, TRACK 5: Top 10 Trigrams for 'You're On Your Own, Kid': [(4, ('your', 'own', 'kid')), (4, (\"you're\", 'on', 'your')), (4, ('on', 'your', 'own')), (3, ('you', 'always', 'have')), (3, ('to', 'fireplace', 'ashes')), (3, ('sprinkler', 'splashes', 'to')), (3, ('splashes', 'to', 'fireplace')), (3, ('own', 'kid', 'you')), (3, ('kid', 'you', 'always')), (3, ('from', 'sprinkler', 'splashes'))]\n",
      "ALBUM 10, TRACK 6: Top 10 Trigrams for 'Midnight Rain': [(4, ('was', 'making', 'my')), (4, ('wanted', 'that', 'pain')), (4, ('wanted', 'it', 'comfortable')), (4, ('wanted', 'a', 'bride')), (4, ('the', 'same', 'all')), (4, ('that', 'pain', 'he')), (4, ('that', 'fame', 'he')), (4, ('stayed', 'the', 'same')), (4, ('same', 'all', 'of')), (4, ('rain', 'he', 'wanted'))]\n",
      "ALBUM 10, TRACK 7: Top 10 Trigrams for 'Question...?': [(4, ('you', 'a', 'question')), (4, ('was', 'making', 'fun')), (4, ('of', 'a', 'fight')), (4, ('more', 'of', 'a')), (4, ('making', 'fun', 'of')), (4, ('in', 'a', 'crowded')), (4, ('i', 'ask', 'you')), (4, ('fun', 'of', 'you')), (4, ('can', 'i', 'ask')), (4, ('ask', 'you', 'a'))]\n",
      "ALBUM 10, TRACK 8: Top 10 Trigrams for 'Vigilante Shit': [(6, (\"don't\", 'dress', 'for')), (5, ('i', \"don't\", 'dress')), (4, ('dressing', 'for', 'revenge')), (4, ('been', 'dressing', 'for')), (3, ('you', 'how', 'it')), (3, ('tell', 'you', 'how')), (3, ('start', 'shit', 'but')), (3, ('so', 'on', 'the')), (3, ('sad', 'get', 'even')), (3, ('on', 'the', 'weekends'))]\n",
      "ALBUM 10, TRACK 9: Top 10 Trigrams for 'Bejeweled': [(7, ('i', 'polish', 'up')), (6, ('polish', 'up', 'real')), (4, ('whole', 'place', 'shimmer')), (4, ('when', 'i', 'walk')), (4, ('walk', 'in', 'the')), (4, ('the', 'whole', 'place')), (4, ('the', 'room', 'i')), (4, ('still', 'make', 'the')), (4, ('still', 'bejeweled', 'when')), (4, ('room', 'i', 'can'))]\n",
      "ALBUM 10, TRACK 10: Top 10 Trigrams for 'Labyrinth': [(21, ('falling', 'in', 'love')), (15, (\"i'm\", 'falling', 'in')), (10, ('oh', \"i'm\", 'falling')), (7, ('in', 'love', 'again')), (5, ('you', 'turn', 'it')), (5, ('was', 'going', 'down')), (5, ('uh', 'oh', \"i'm\")), (5, ('turn', 'it', 'right')), (5, ('thought', 'the', 'plane')), (5, ('the', 'plane', 'was'))]\n",
      "ALBUM 10, TRACK 11: Top 10 Trigrams for 'Karma': [(9, ('karma', 'is', 'a')), (5, ('my', 'boyfriend', 'karma')), (5, ('karma', 'is', 'the')), (5, ('karma', 'is', 'my')), (5, ('is', 'my', 'boyfriend')), (5, ('boyfriend', 'karma', 'is')), (4, ('is', 'a', 'god')), (4, ('a', 'relaxing', 'thought')), (4, (\"'cause\", 'karma', 'is')), (3, ('you', \"it's\", 'not'))]\n",
      "ALBUM 10, TRACK 12: Top 10 Trigrams for 'Sweet Nothing': [(5, (\"you're\", 'in', 'the')), (5, ('up', 'to', 'something')), (5, (\"they're\", 'push', 'and')), (5, ('they', 'said', 'the')), (5, ('the', 'kitchen', 'humming')), (5, ('the', 'end', 'is')), (5, ('said', 'the', 'end')), (5, ('push', 'and', 'shoving')), (5, ('outside', \"they're\", 'push')), (5, ('in', 'the', 'kitchen'))]\n",
      "ALBUM 10, TRACK 13: Top 10 Trigrams for 'Mastermind': [(6, (\"i'm\", 'a', 'mastermind')), (5, ('i', 'told', 'you')), (4, ('what', 'if', 'i')), (4, ('if', 'i', 'told')), (3, ('you', 'saw', 'me')), (3, ('you', 'none', 'of')), (3, ('was', 'accidental', 'and')), (3, ('told', 'you', 'none')), (3, ('the', 'groundwork', 'and')), (3, ('the', 'first', 'night'))]\n"
     ]
    }
   ],
   "source": [
    "#TOP 10 TRIGRAMS FOR EACH SONG\n",
    "\n",
    "import string\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "from nltk.tokenize import sent_tokenize  \n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(\"['\\w']+\")\n",
    "\n",
    "def trigram_list(file):    \n",
    "    fin = open(file) \n",
    "    s = fin.read()\n",
    "    \n",
    "    list_of_trigrams = []\n",
    "    for sent in sent_tokenize(s):\n",
    "        sent = sent.lower()\n",
    "        tokens = tokenizer.tokenize(sent)\n",
    "        res = list(nltk.trigrams(tokens))\n",
    "        list_of_trigrams.extend(res)\n",
    "\n",
    "    d = dict()\n",
    "    for t in list_of_trigrams:\n",
    "        if t not in d:\n",
    "            d[t] = 1\n",
    "        else:\n",
    "            d[t] += 1\n",
    "    lst = []\n",
    "    for key, val in d.items():\n",
    "        t = (val, key)\n",
    "        lst.append(t)\n",
    "    lst.sort(reverse=True)\n",
    "    return lst[:10]\n",
    "\n",
    "t = trigram_list(\"Songs/Taylor Swift (2006)/ts1t1_tim_mcgraw.txt\")\n",
    "print(f\"ALBUM 1, TRACK 1: Top 10 Trigrams for 'Tim McGraw': {t}\")\n",
    "\n",
    "t = trigram_list(\"Songs/Taylor Swift (2006)/ts1t2_picture_to_burn.txt\")\n",
    "print(f\"ALBUM 1, TRACK 2: Top 10 Trigrams for 'Picture to Burn': {t}\")\n",
    "\n",
    "t = trigram_list(\"Songs/Taylor Swift (2006)/ts1t3_teardrops_on_my_guitar.txt\")\n",
    "print(f\"ALBUM 1, TRACK 3: Top 10 Trigrams for 'Teardrops On My Guitar': {t}\")\n",
    "\n",
    "t = trigram_list(\"Songs/Taylor Swift (2006)/ts1t4_a_place_in_this_world.txt\")\n",
    "print(f\"ALBUM 1, TRACK 4: Top 10 Trigrams for 'A Place In This World': {t}\")\n",
    "\n",
    "t = trigram_list(\"Songs/Taylor Swift (2006)/ts1t5_cold_as_you.txt\")\n",
    "print(f\"ALBUM 1, TRACK 5: Top 10 Trigrams for 'Cold As You': {t}\")\n",
    "\n",
    "t = trigram_list(\"Songs/Taylor Swift (2006)/ts1t6_the_outside.txt\")\n",
    "print(f\"ALBUM 1, TRACK 6: Top 10 Trigrams for 'The Outside': {t}\")\n",
    "\n",
    "t = trigram_list(\"Songs/Taylor Swift (2006)/ts1t7_tied_together_with_a_smile.txt\")\n",
    "print(f\"ALBUM 1, TRACK 7: Top 10 Trigrams for 'Tied Together With a Smile': {t}\")\n",
    "\n",
    "t = trigram_list(\"Songs/Taylor Swift (2006)/ts1t8_stay_beautiful.txt\")\n",
    "print(f\"ALBUM 1, TRACK 8: Top 10 Trigrams for 'Stay Beautiful': {t}\")\n",
    "\n",
    "t = trigram_list(\"Songs/Taylor Swift (2006)/ts1t9_shouldve_said_no.txt\")\n",
    "print(f\"ALBUM 1, TRACK 9: Top 10 Trigrams for 'Should've Said No': {t}\")\n",
    "\n",
    "t = trigram_list(\"Songs/Taylor Swift (2006)/ts1t10_marys_song.txt\")\n",
    "print(f\"ALBUM 1, TRACK 10: Top 10 Trigrams for 'Mary's Song': {t}\")\n",
    "\n",
    "t = trigram_list(\"Songs/Taylor Swift (2006)/ts1t11_our_song.txt\")\n",
    "print(f\"ALBUM 1, TRACK 11: Top 10 Trigrams for 'Our Song': {t}\")\n",
    "\n",
    "print(   )\n",
    "\n",
    "t = trigram_list(\"Songs/Midnights (2022)/ts10t1_lavender_haze.txt\")\n",
    "print(f\"ALBUM 10, TRACK 1: Top 10 Trigrams for 'Lavender Haze': {t}\")\n",
    "\n",
    "t = trigram_list(\"Songs/Midnights (2022)/ts10t2_maroon.txt\")\n",
    "print(f\"ALBUM 10, TRACK 2: Top 10 Trigrams for 'Maroon': {t}\")\n",
    "\n",
    "t = trigram_list(\"Songs/Midnights (2022)/ts10t3_anti_hero.txt\")\n",
    "print(f\"ALBUM 10, TRACK 3: Top 10 Trigrams for 'Anti_Hero': {t}\")\n",
    "\n",
    "t = trigram_list(\"Songs/Midnights (2022)/ts10t4_snow_on_the_beach.txt\")\n",
    "print(f\"ALBUM 10, TRACK 4: Top 10 Trigrams for 'Snow On The Beach': {t}\")\n",
    "     \n",
    "t = trigram_list(\"Songs/Midnights (2022)/ts10t5_youre_on_your_own_kid.txt\")\n",
    "print(f\"ALBUM 10, TRACK 5: Top 10 Trigrams for 'You're On Your Own, Kid': {t}\")\n",
    "\n",
    "t = trigram_list(\"Songs/Midnights (2022)/ts10t6_midnight_rain.txt\")\n",
    "print(f\"ALBUM 10, TRACK 6: Top 10 Trigrams for 'Midnight Rain': {t}\")\n",
    "\n",
    "t = trigram_list(\"Songs/Midnights (2022)/ts10t7_question.txt\")\n",
    "print(f\"ALBUM 10, TRACK 7: Top 10 Trigrams for 'Question...?': {t}\")\n",
    "\n",
    "t = trigram_list(\"Songs/Midnights (2022)/ts10t8_vigilante_shit.txt\")\n",
    "print(f\"ALBUM 10, TRACK 8: Top 10 Trigrams for 'Vigilante Shit': {t}\")\n",
    "\n",
    "t = trigram_list(\"Songs/Midnights (2022)/ts10t9_bejeweled.txt\")\n",
    "print(f\"ALBUM 10, TRACK 9: Top 10 Trigrams for 'Bejeweled': {t}\")\n",
    "\n",
    "t = trigram_list(\"Songs/Midnights (2022)/ts10t10_labyrinth.txt\")\n",
    "print(f\"ALBUM 10, TRACK 10: Top 10 Trigrams for 'Labyrinth': {t}\")\n",
    "\n",
    "t = trigram_list(\"Songs/Midnights (2022)/ts10t11_karma.txt\")\n",
    "print(f\"ALBUM 10, TRACK 11: Top 10 Trigrams for 'Karma': {t}\")\n",
    "\n",
    "t = trigram_list(\"Songs/Midnights (2022)/ts10t12_sweet_nothing.txt\")\n",
    "print(f\"ALBUM 10, TRACK 12: Top 10 Trigrams for 'Sweet Nothing': {t}\")\n",
    "\n",
    "t = trigram_list(\"Songs/Midnights (2022)/ts10t13_mastermind.txt\")\n",
    "print(f\"ALBUM 10, TRACK 13: Top 10 Trigrams for 'Mastermind': {t}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc4d1a1",
   "metadata": {},
   "source": [
    "## 2.1 - Average type-token ratio (TTR) for each group (album)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5cf3a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type-token ratio for 'Taylor Swift (2006)' is 0.0035233421416886877.\n",
      "\n",
      "The type-token ratio for 'Midnights (2022)' is 0.0030329488534534257.\n"
     ]
    }
   ],
   "source": [
    "#TTR PER ALBUM\n",
    "\n",
    "import pathlib\n",
    "path = pathlib.Path(\"Songs/Taylor Swift (2006)\")\n",
    "files = path.glob(\"*.txt\")\n",
    "x = \"\"\n",
    "for file in files:\n",
    "    f = open(file)\n",
    "    x += (f.read())\n",
    "\n",
    "def get_ttr(list_of_words):\n",
    "    unique_words = []\n",
    "    \n",
    "    for word in list_of_words:\n",
    "        if word not in unique_words:\n",
    "            unique_words.append(word)\n",
    "               \n",
    "    ttr = len(unique_words)/len(list_of_words)\n",
    "    return ttr\n",
    "\n",
    "m = get_ttr(x)\n",
    "print(f\"The type-token ratio for 'Taylor Swift (2006)' is {m}.\")\n",
    "print(    )\n",
    "\n",
    "import pathlib\n",
    "path = pathlib.Path(\"Songs/Midnights (2022)\")\n",
    "files = path.glob(\"*.txt\")\n",
    "x = \"\"\n",
    "for file in files:\n",
    "    f = open(file)\n",
    "    x += (f.read())\n",
    "    \n",
    "m = get_ttr(x)\n",
    "print(f\"The type-token ratio for 'Midnights (2022)' is {m}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee22bce",
   "metadata": {},
   "source": [
    "## 2.2 - Average word length per group (album)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "619ceb2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average word length for 'Midnights' (2022) is 4.1180620884289745 characters.\n",
      "\n",
      "Average word length for 'Taylor Swift' (2006) is 3.9486301369863015 characters.\n"
     ]
    }
   ],
   "source": [
    "#AVG WORD LENGTH PER ALBUM\n",
    "\n",
    "import pathlib\n",
    "path = pathlib.Path(\"Songs/Midnights (2022)\")\n",
    "files = path.glob(\"*.txt\")\n",
    "x = \"\"\n",
    "for file in files:\n",
    "    f = open(file)\n",
    "    x += (f.read())\n",
    "\n",
    "def make_word_list_album(x):\n",
    "    words = x.split()\n",
    "    return words\n",
    "\n",
    "def avg_word_length_album(x):\n",
    "    words = make_word_list_album(x)\n",
    "    average = sum(len(word) for word in words) / len(words)\n",
    "    return average\n",
    "\n",
    "m = avg_word_length_album(x)\n",
    "print(f\"Average word length for 'Midnights' (2022) is {m} characters.\")\n",
    "print(    )\n",
    "\n",
    "path = pathlib.Path(\"Songs/Taylor Swift (2006)\")\n",
    "files = path.glob(\"*.txt\")\n",
    "x = \"\"\n",
    "for file in files:\n",
    "    f = open(file)\n",
    "    x += (f.read())\n",
    "\n",
    "m = avg_word_length_album(x)\n",
    "print(f\"Average word length for 'Taylor Swift' (2006) is {m} characters.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91683872",
   "metadata": {},
   "source": [
    "## 2.3 - Average number of words per document for each group (album)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "23ee9588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALBUM 1: Average number of words per song on 'Taylor Swift' (2006): 247.07692307692307\n",
      "\n",
      "ALBUM 10: Average number of words per song on 'Midnights' (2022): 327.0769230769231\n"
     ]
    }
   ],
   "source": [
    "#AVG NUMBER WORDS PER SONG PER ALBUM\n",
    "\n",
    "import pathlib\n",
    "path = pathlib.Path(\"Songs/Taylor Swift (2006)\")\n",
    "files = path.glob(\"*.txt\")\n",
    "x = \"\"\n",
    "for file in files:\n",
    "    f = open(file)\n",
    "    x += (f.read())\n",
    "    words = x.split()\n",
    "print(f\"ALBUM 1: Average number of words per song on 'Taylor Swift' (2006): {len(words)/13}\")\n",
    "print(    )\n",
    "\n",
    "import pathlib\n",
    "path = pathlib.Path(\"Songs/Midnights (2022)\")\n",
    "files = path.glob(\"*.txt\")\n",
    "x = \"\"\n",
    "for file in files:\n",
    "    f = open(file)\n",
    "    x += (f.read())\n",
    "    words = x.split()\n",
    "print(f\"ALBUM 10: Average number of words per song on 'Midnights' (2022): {len(words)/13}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed337fd",
   "metadata": {},
   "source": [
    "## 2.4 - Top 10 most frequently used words (excluding function words) per group (album)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4cd61e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALBUM 1: Top 10 Non-Function Words on 'Taylor Swift' (2006): \n",
      "[('oh,', 31), (\"you're\", 30), (\"i'm\", 29), ('think', 24), ('back', 24), ('just', 21), (\"don't\", 18), ('know', 18), (\"should've\", 18), ('said,', 17)]\n",
      "ALBUM 10: Top 10 Non-Function Words on 'Midnights' (2022): \n",
      "[('like', 50), (\"it's\", 50), (\"i'm\", 47), ('just', 27), (\"don't\", 25), ('karma', 23), ('love', 22), ('coming', 22), (\"'cause\", 21), ('down,', 20)]\n"
     ]
    }
   ],
   "source": [
    "#TOP10 MOST USED WORDS PER ALBUM\n",
    "\n",
    "file = open(\"function_words.txt\")\n",
    "data = file.read()\n",
    "function_words = data.split()\n",
    "\n",
    "def get_most_freq_word_album(a):\n",
    "    d = dict()\n",
    "    for word in a:\n",
    "        if word in function_words:\n",
    "            continue\n",
    "        if word not in d:\n",
    "            d[word] = 1\n",
    "        else:\n",
    "            d[word] += 1\n",
    "    s = sorted(d.items(), key=lambda item:-item[1])\n",
    "    return s[:10]\n",
    "\n",
    "import pathlib\n",
    "path = pathlib.Path(\"Songs/Taylor Swift (2006)\")\n",
    "files = path.glob(\"*.txt\")\n",
    "x = \"\"\n",
    "for file in files:\n",
    "    f = open(file)\n",
    "    x += (f.read())\n",
    "    words_ts = x.lower().split()\n",
    "    \n",
    "import pathlib\n",
    "path = pathlib.Path(\"Songs/Midnights (2022)\")\n",
    "files = path.glob(\"*.txt\")\n",
    "x = \"\"\n",
    "for file in files:\n",
    "    f = open(file)\n",
    "    x += (f.read())\n",
    "    words_m = x.lower().split()\n",
    "\n",
    "ts = get_most_freq_word_album(words_ts)\n",
    "print(f\"ALBUM 1: Top 10 Non-Function Words on 'Taylor Swift' (2006): \\n{ts}\")\n",
    "m = get_most_freq_word_album(words_m)\n",
    "print(f\"ALBUM 10: Top 10 Non-Function Words on 'Midnights' (2022): \\n{m}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3d34da",
   "metadata": {},
   "source": [
    "## 2.5 - Top 10 most frequently used bigrams, and trigrams (including function words) for each group (album)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "147013a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALBUM 1: Top 10 Bigrams for 'Taylor Swift' (2006): \n",
      "[(18, ('you', 'think')), (17, ('and', 'i')), (15, ('you', \"should've\")), (14, ('when', 'you')), (14, ('oh', 'oh')), (14, ('back', 'to')), (13, ('in', 'the')), (13, ('i', 'hope')), (12, ('on', 'the')), (11, ('on', 'my'))]\n",
      "\n",
      "ALBUM 10: Top 10 Bigrams for 'Midnights' (2022): \n",
      "[(27, ('in', 'the')), (22, ('on', 'the')), (22, ('it', 'was')), (21, ('in', 'love')), (21, ('falling', 'in')), (20, ('karma', 'is')), (17, ('like', 'snow')), (17, (\"it's\", 'coming')), (15, (\"i'm\", 'falling')), (15, ('coming', 'down'))]\n"
     ]
    }
   ],
   "source": [
    "#TOP 10 BIGRAMS FOR EACH ALBUM\n",
    "\n",
    "import pathlib\n",
    "import string\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "from nltk.tokenize import sent_tokenize  \n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(\"['\\w']+\")\n",
    "\n",
    "def bigram_list_album(album):    \n",
    "    path = pathlib.Path(album)\n",
    "    files = path.glob(\"*.txt\")\n",
    "    x = \"\"\n",
    "    for file in files:\n",
    "        f = open(file)\n",
    "        x += (f.read())\n",
    "    words = x.lower()\n",
    "    \n",
    "    list_of_bigrams = []\n",
    "    for sent in sent_tokenize(words):\n",
    "        sent = sent.lower()\n",
    "        tokens = tokenizer.tokenize(sent)\n",
    "        res = list(nltk.bigrams(tokens))\n",
    "        list_of_bigrams.extend(res)\n",
    "\n",
    "    d = dict()\n",
    "    for t in list_of_bigrams:\n",
    "        if t not in d:\n",
    "            d[t] = 1\n",
    "        else:\n",
    "            d[t] += 1\n",
    "    lst = []\n",
    "    for key, val in d.items():\n",
    "        t = (val, key)\n",
    "        lst.append(t)\n",
    "    lst.sort(reverse=True)\n",
    "    return lst[:10]\n",
    "\n",
    " \n",
    "b1 = bigram_list_album(\"Songs/Taylor Swift (2006)\")\n",
    "print(f\"ALBUM 1: Top 10 Bigrams for 'Taylor Swift' (2006): \\n{b1}\")\n",
    "print (    )\n",
    "\n",
    "b10 = bigram_list_album(\"Songs/Midnights (2022)\")\n",
    "print(f\"ALBUM 10: Top 10 Bigrams for 'Midnights' (2022): \\n{b10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "53fa5fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALBUM 1: Top 10 Trigrams for 'Taylor Swift' (2006): \n",
      "[(10, ('oh', 'oh', 'oh')), (9, ('when', 'you', 'think')), (9, ('i', 'hope', 'you')), (9, ('hope', 'you', 'think')), (7, (\"i'm\", 'just', 'a')), (6, ('you', 'think', 'tim')), (6, ('you', \"should've\", 'said')), (6, ('tim', 'mcgraw', 'i')), (6, ('think', 'tim', 'mcgraw')), (6, ('song', 'is', 'the'))]\n",
      "\n",
      "ALBUM 10: Top 10 Trigrams for 'Midnights' (2022): \n",
      "[(21, ('falling', 'in', 'love')), (15, (\"it's\", 'coming', 'down')), (15, (\"i'm\", 'falling', 'in')), (13, ('down', \"it's\", 'coming')), (12, ('coming', 'down', \"it's\")), (11, ('snow', 'on', 'the')), (11, ('on', 'the', 'beach')), (11, ('like', 'snow', 'on')), (10, ('oh', \"i'm\", 'falling')), (9, ('the', 'beach', 'like'))]\n"
     ]
    }
   ],
   "source": [
    "#TOP 10 TRIGRAMS FOR EACH ALBUM\n",
    "\n",
    "import pathlib\n",
    "import string\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "from nltk.tokenize import sent_tokenize  \n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(\"['\\w']+\")\n",
    "\n",
    "def trigram_list_album(album):    \n",
    "    path = pathlib.Path(album)\n",
    "    files = path.glob(\"*.txt\")\n",
    "    x = \"\"\n",
    "    for file in files:\n",
    "        f = open(file)\n",
    "        x += (f.read())\n",
    "    words = x.lower()\n",
    "    \n",
    "    list_of_trigrams = []\n",
    "    for sent in sent_tokenize(words):\n",
    "        sent = sent.lower()\n",
    "        tokens = tokenizer.tokenize(sent)\n",
    "        res = list(nltk.trigrams(tokens))\n",
    "        list_of_trigrams.extend(res)\n",
    "\n",
    "    d = dict()\n",
    "    for t in list_of_trigrams:\n",
    "        if t not in d:\n",
    "            d[t] = 1\n",
    "        else:\n",
    "            d[t] += 1\n",
    "    lst = []\n",
    "    for key, val in d.items():\n",
    "        t = (val, key)\n",
    "        lst.append(t)\n",
    "    lst.sort(reverse=True)\n",
    "    return lst[:10]\n",
    "\n",
    " \n",
    "t1 = trigram_list_album(\"Songs/Taylor Swift (2006)\")\n",
    "print(f\"ALBUM 1: Top 10 Trigrams for 'Taylor Swift' (2006): \\n{t1}\")\n",
    "print (    )\n",
    "\n",
    "t10 = trigram_list_album(\"Songs/Midnights (2022)\")\n",
    "print(f\"ALBUM 10: Top 10 Trigrams for 'Midnights' (2022): \\n{t10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a997af6",
   "metadata": {},
   "source": [
    "## 3 - Put it in a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "45cebd14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "724"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CSV FILE\n",
    "\n",
    "import csv\n",
    "csv_generator = open(\"taylorswift.csv\", \"w\")\n",
    "writer = csv.DictWriter(csv_generator, fieldnames=[\"Files\", \"TTR\", \"Word Length\", \"Word Count\", \"Top 10 Words\", \"Top 10 Bigrams\", \"Top 10 Trigrams\"])\n",
    "writer.writeheader()\n",
    "\n",
    "def make_dict(path, name):\n",
    "    d = dict()\n",
    "    words = make_word_list(path)\n",
    "    d[\"Files\"] = name\n",
    "    d[\"TTR\"] = get_ttr(words)\n",
    "    d[\"Word Length\"] = avg_word_length(path)\n",
    "    d[\"Word Count\"] = len(words)\n",
    "    d[\"Top 10 Words\"] = get_most_freq_word(path)\n",
    "    d[\"Top 10 Bigrams\"] = bigram_list(path)\n",
    "    d[\"Top 10 Trigrams\"] = trigram_list(path)\n",
    "    return d\n",
    "    \n",
    "def make_dict_album(path, name):\n",
    "    album = pathlib.Path(path)\n",
    "    files = album.glob(\"*.txt\")\n",
    "    x = \"\"\n",
    "    for file in files:\n",
    "        f = open(file)\n",
    "        x += (f.read())\n",
    "    words = x.lower().split()\n",
    "    d = dict()\n",
    "    d[\"Files\"] = name\n",
    "    d[\"TTR\"] = get_ttr(words)\n",
    "    d[\"Word Length\"] = avg_word_length_album(x)\n",
    "    d[\"Word Count\"] = len(words)\n",
    "    d[\"Top 10 Words\"] = get_most_freq_word_album(words)\n",
    "    d[\"Top 10 Bigrams\"] = bigram_list_album(path)\n",
    "    d[\"Top 10 Trigrams\"] = trigram_list_album(path)\n",
    "    return d\n",
    "\n",
    "writer.writerow(make_dict_album(\"Songs/Taylor Swift (2006)\", \"Taylor Swift (2006)\"))\n",
    "writer.writerow(make_dict(\"Songs/Taylor Swift (2006)/ts1t1_tim_mcgraw.txt\", \"Tim McGraw\"))\n",
    "writer.writerow(make_dict(\"Songs/Taylor Swift (2006)/ts1t2_picture_to_burn.txt\", \"Picture to Burn\"))\n",
    "writer.writerow(make_dict(\"Songs/Taylor Swift (2006)/ts1t3_teardrops_on_my_guitar.txt\", \"Teardrops On My Guitar\"))\n",
    "writer.writerow(make_dict(\"Songs/Taylor Swift (2006)/ts1t4_a_place_in_this_world.txt\", \"A Place In This World\"))\n",
    "writer.writerow(make_dict(\"Songs/Taylor Swift (2006)/ts1t5_cold_as_you.txt\", \"Cold As You\"))\n",
    "writer.writerow(make_dict(\"Songs/Taylor Swift (2006)/ts1t6_the_outside.txt\", \"The Outside\"))\n",
    "writer.writerow(make_dict(\"Songs/Taylor Swift (2006)/ts1t7_tied_together_with_a_smile.txt\", \"Tied Together With a Smile\"))\n",
    "writer.writerow(make_dict(\"Songs/Taylor Swift (2006)/ts1t8_stay_beautiful.txt\", \"Stay Beautiful\"))\n",
    "writer.writerow(make_dict(\"Songs/Taylor Swift (2006)/ts1t9_shouldve_said_no.txt\", \"Should've Said No\"))\n",
    "writer.writerow(make_dict(\"Songs/Taylor Swift (2006)/ts1t10_marys_song.txt\", \"Mary's Song\"))\n",
    "writer.writerow(make_dict(\"Songs/Taylor Swift (2006)/ts1t11_our_song.txt\", \"Our Song\"))\n",
    "\n",
    "writer.writerow(make_dict_album(\"Songs/Midnights (2022)\", \"Midnights (2022)\"))\n",
    "writer.writerow(make_dict(\"Songs/Midnights (2022)/ts10t1_lavender_haze.txt\", \"Lavender Haze\"))\n",
    "writer.writerow(make_dict(\"Songs/Midnights (2022)/ts10t2_maroon.txt\", \"Maroon\"))\n",
    "writer.writerow(make_dict(\"Songs/Midnights (2022)/ts10t3_anti_hero.txt\", \"Anti-Hero\"))\n",
    "writer.writerow(make_dict(\"Songs/Midnights (2022)/ts10t4_snow_on_the_beach.txt\", \"Snow On The Beach\"))\n",
    "writer.writerow(make_dict(\"Songs/Midnights (2022)/ts10t5_youre_on_your_own_kid.txt\", \"You're On Your Own, Kid\"))\n",
    "writer.writerow(make_dict(\"Songs/Midnights (2022)/ts10t6_midnight_rain.txt\", \"Midnight Rain\"))\n",
    "writer.writerow(make_dict(\"Songs/Midnights (2022)/ts10t7_question.txt\", \"Question...?\"))\n",
    "writer.writerow(make_dict(\"Songs/Midnights (2022)/ts10t8_vigilante_shit.txt\", \"Vigilante Shit\"))\n",
    "writer.writerow(make_dict(\"Songs/Midnights (2022)/ts10t9_bejeweled.txt\", \"Bejeweled\"))\n",
    "writer.writerow(make_dict(\"Songs/Midnights (2022)/ts10t10_labyrinth.txt\", \"Labyrinth\"))\n",
    "writer.writerow(make_dict(\"Songs/Midnights (2022)/ts10t11_karma.txt\", \"Karma\"))\n",
    "writer.writerow(make_dict(\"Songs/Midnights (2022)/ts10t12_sweet_nothing.txt\", \"Sweet Nothing\"))\n",
    "writer.writerow(make_dict(\"Songs/Midnights (2022)/ts10t13_mastermind.txt\", \"Mastermind\"))\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
